.. _samplemethods:

		  		   
SampleMethods
=============

This module contains functionality for all the sampling methods supported in ``UQpy``. 
The module currently contains the following classes:

- ``MCS``: Class to perform Monte Carlo sampling using ``UQpy``.
- ``LHS``: Class to perform Latin hypercube sampling using ``UQpy``.
- ``MCMC``: Class to perform sampling using Markov chains using ``UQpy``.
- ``IS``: Class to perform Importance sampling using ``UQpy``.


MCS
----

``MCS``  class can be used to generate  random  draws  from  specified probability distribution(s).  The ``MCS``
class utilizes the ``Distributions`` class to define probability distributions.  The advantage of using the ``MCS``
class for ``UQpy`` operations, as opposed to simply generating samples with the ``scipy.stats`` package, is that it
allows building an object  containing  the  samples,  their  distributions  and variable names for integration with
other ``UQpy`` modules.

``MCS``  class can be imported in a python script using the following command:

>>> from UQpy.SampleMethods import MCS

For example,  to run MCS  for two independent normally distribution random variables `N(1,1)` and `N(0,1)`

>>> from UQpy.Distributions import Normal
>>> dist1 = Normal(loc=1., scale=1.)
>>> dist2 = Normal(loc=0., scale=1.)
>>> x1 = MCS(dist_object=[dist1, dist2], nsamples=5, random_state = [1,3], verbose=True)
>>> print(x1.samples)
    UQpy: Running Monte Carlo Sampling...
    UQpy: Monte Carlo Sampling Complete.
	[[ 1.62434536  1.78862847]
 	[-0.61175641  0.43650985]
 	[-0.52817175  0.09649747]
	[-1.07296862 -1.8634927 ]
 	[ 0.86540763 -0.2773882 ]]

The ``MCS`` class can be used to run MCS for multivariate distributions

>>> from UQpy.Distributions import MVNormal
>>> dist = MVNormal(mean=[1., 2.], cov=[[4., -0.9], [-0.9, 1.]])
>>> x2 = MCS(dist_object=[dist], nsamples=5, random_state=123)
>>> print(x2.samples)
	[[ 3.38736185  2.23541269]
	[ 0.08946208  0.8979547 ]
	[ 2.53138343  3.06057229]
	[ 5.72159837  0.30657467]
	[-1.71534735  1.97285583]]

Or for a combination of distributions

>>> from UQpy.Distributions import MVNormal, Normal
>>> dist1 = Normal(loc=1., scale=1.)
>>> dist = MVNormal(mean=[1., 2.], cov=[[4., -0.9], [-0.9, 1.]])
>>> x3 = MCS(dist_object=[dist1, dist], nsamples=5, random_state=[123, None])
>>> print(x3.samples)
	[[array([-1.0856306]) array([0.21193807, 2.35155014])]
 	[array([0.99734545]) array([-1.02985401,  1.83075511])]
 	[array([0.2829785]) array([3.09845703, 1.67722522])]
 	[array([-1.50629471]) array([2.13964859, 1.22068072])]
 	[array([-0.57860025]) array([-1.16164199,  2.21637435])]]
	 
In this case the number of  samples will be

>>> print(len(x3.samples.shape))
    5
and the dimension of the problem is

>>> print(len(x3.samples[0].shape))
    2

.. autoclass:: UQpy.SampleMethods.MCS
	:members:


LHS
----

``LHS``  class can be used to generate  random  draws  from  specified probability distribution(s) using Latin hypercube sampling, which belongs to the family of stratified sampling techniques. LHS has the advantage that the samples generated are uniformly distributed over each marginal distribution. LHS is perfomed by dividing the the range of each random variable into N bins with equal probability mass, where N is the required number of samples, generating one sample per bin, and then randomly pairing the samples.

``LHS``  class can be imported in a python script using the following command:

>>> from UQpy.SampleMethods import LHS

For example,  to run LHS  for two independent uniformly distribution random variables `U(0, 1)`

>>> from UQpy.Distributions import Normal
>>> dist1 = Uniform(loc=0., scale=1.)
>>> dist2 = Uniform(loc=0., scale=1.)
>>> x1 = LHS(dist_object=[dist1, dist2], nsamples=5,  verbose=True)
>>> print(x1.samples)
	UQpy: Running Latin Hypercube sampling...
	Successful execution of LHS design.
	[[0.01373095 0.83176942]
 	[0.34778514 0.52142516]
 	[0.77989405 0.30824438]
 	[0.55000767 0.16585118]
 	[0.9397917  0.6990165 ]]
	 
The ``LHS`` class of ``UQpy`` offers a variaty of methods for a Latin Hypercube Design ('random', 'centered', 'minmax', 'correlate`). However, adding a new method is straightforward. For example, if we want to perform a LHS desing using a new method we can do it easily by providing a function as the `criterion`. The output of this function should be an array at least two-dimension. In the same way, a distance metric can be provided by the user. For example:

	
>>> from UQpy.Distributions import Uniform
>>> dist1 = Uniform(loc=0., scale=1.)
>>> dist2 = Uniform(loc=0., scale=1.)

>>> def new_method():
>>> samples_in_U_ab = np.atleast_2d(np.array([1., 1.]))
>>> 	return samples_in_U_ab

>>> def new_distance(y):
>>> 	return y + 1

>>> x1 = LHS(dist_object=[dist1, dist2], nsamples=5, criterion=new_method, metric=new_distance)
>>> print(x1.samples)
	[[1. 1.]
 	[1. 1.]
 	[1. 1.]
 	[1. 1.]
 	[1. 1.]]


.. autoclass:: UQpy.SampleMethods.LHS
	:members:

	
MCMC
----

The goal of Markov Chain Monte Carlo is to draw samples from some probability distribution :math:`p(x)=\frac{\tilde{p}(x)}{Z}`, where :math:`\tilde{p}(x)` is known but :math:`Z` is hard to compute (this will often be the case when using Bayes' theorem for instance). In order to do this, the theory of a Markov chain, a stochastic model that describes a sequence of states in which the probability of a state depends only on the previous state, is combined with a Monte Carlo simulation method, see e.g. ([1]_, [2]_). More specifically, a Markov Chain is built and sampled from whose stationary distribution is the target distribution :math:`p(x)`.  For instance, the Metropolis-Hastings (MH) algorithm goes as follows:

* initialize with a seed sample :math:`x_{0}`
* walk the chain: for :math:`k=0,...` do:
   * sample candidate :math:`x^{\star} \sim Q(\cdot \vert x_{k})` for a given Markov transition probability :math:`Q`
   * accept candidate (set :math:`x_{k+1}=x^{\star}`) with probability :math:`\alpha(x^{\star} \vert x_{k})`, otherwise propagate last sample :math:`x_{k+1}=x_{k}`.
   
.. math:: \alpha(x^{\star} \vert x_{k}):= \min \left\{ \frac{\tilde{p}(x^{\star})}{\tilde{p}(x)}\cdot \frac{Q(x \vert x^{\star})}{Q(x^{\star} \vert x)}, 1 \right\}
     
The transition probability :math:`Q` is chosen by the user (see input `proposal` of the MH algorithm, and careful attention must be given to that choice as it plays a major role in the accuracy and efficiency of the algorithm. The following figure shows samples accepted (blue) and rejected (red) when trying to sample from a 2d Gaussian distribution using MH, for different scale parameters of the proposal distribution. If the scale is too small, the space is not well explored; if the scale is too large, many candidate samples will be rejected, yielding a very inefficient algorithm. As a rule of thumb, an acceptance rate of 10\%-50\% could be targeted (see `Diagnostics` in the `Utilities` module).

.. image:: _static/SampleMethods_MCMC_samples.png
   :scale: 40 %
   :alt: IS weighted samples
   :align: center

Finally, samples from the target distribution will be generated only when the chain has converged to its stationary distribution, after a so-called burn-in period. Thus the user would often reject the first few samples (see input `nburn`). Also, the chain yields correlated samples; thus to obtain i.i.d. samples from the target distribution, the user should keep only one out of n samples (see input `jump`). This means that the code will perform in total nburn + jump * N evaluations of the target pdf to yield N i.i.d. samples from the target distribution (for the MH algorithm with a single chain).

The parent class for all MCMC algorithms is the ``MCMC class``, which defines the inputs that are common to all MCMC algorithms, along with the *run* method that is being called to run the chain. Any given MCMC algorithm is a sub-class of MCMC that overwrites the main *run_one_iteration* method.

.. autoclass:: UQpy.SampleMethods.MCMC
   :members:

MH
~~~~~

.. autoclass:: UQpy.SampleMethods.MH
	:members:

MMH
~~~~~
   
.. autoclass:: UQpy.SampleMethods.MMH
	:members:

Stretch
~~~~~~~~
   
.. autoclass:: UQpy.SampleMethods.Stretch
	:members:

DRAM
~~~~~~~
   
.. autoclass:: UQpy.SampleMethods.DRAM
	:members:

DREAM
~~~~~~~
   
.. autoclass:: UQpy.SampleMethods.DREAM
	:members:

Adding a new MCMC 
~~~~~~~~~~~~~~~~~~~~~

In order to add a new MCMC algorithm, a user must create a subclass of ``MCMC``, and overwrite the *run_one_iteration* method that propagates all the chains forward one iteration. Such a new class may use any number of additional inputs compared to the ``MCMC`` base class. The reader is encouraged to have a look at the ``MH`` class and its code to better understand how a particular algorithm should fit the general framework. 

A useful note is that the user has access to a number of useful attributes / utility methods as the algorithm proceeds, such as:

* the attribute `evaluate_log_target` (and possibly `self.evaluate_log_target_marginals` if marginals were provided) is created at initialization. It is a callable that simply evaluates the log-pdf of the target distribution at a given point `x`. It can be called within the code of a new sampler as `log_pdf_value = self.evaluate_log_target(x)`. 
* the `nsamples` and `nsamples_per_chain` attributes indicate the number of samples that have been stored up to the current iteration (i.e., they are updated dynamically as the algorithm proceeds),
* the `samples` attribute contains all previously stored samples. Cautionary note: `self.samples` also contains trailing zeros, for samples yet to be stored, thus to access all previously stored samples at a given iteration the user must call `self.samples[:self.nsamples_per_chain]`, which will return a (self.nsamples_per_chain, self.nchains, self.dimension) `ndarray`,
* the `log_pdf_values` attribute contains all previously stored log target values, same cautionary note as above,
* the `_update_acceptance_rate` method updates the `acceptance_rate` attribute of the sampler, given a  (list of) boolean(s) indicating if the candidate state(s) were accepted at a given iteration,
* the `_check_methods_proposal` method checks wether a given proposal is adequate (i.e., has `rvs` and `log_pdf`/`pdf` methods).

   
IS
----

Importance sampling (IS) is based on the idea of concentrating sampling in certain regions of the input space, allowing efficient evaluations of expectations :math:`E_{ \textbf{x} \sim p} [ f(\textbf{x}) ]` where :math:`f( \textbf{x})` is small outside of a small region of the input space. To this end, a sample :math:`\textbf{x}` is drawn from a proposal distribution :math:`q(\textbf{x})` and re-weighted to correct for the discrepancy between the sampling distribution :math:`q` and the true distribution :math:`p`. The weight of the sample is computed as 

.. math:: w(\textbf{x}) = \frac{p(\textbf{x})}{q(\textbf{x})}

If :math:`p` is only known up to a constant, i.e., one can only evaluate :math:`\tilde{p}(\textbf{x})`, where :math:`p(\textbf{x})=\frac{\tilde{p}(\textbf{x})}{Z}`, IS can be used by further normalizing the weights (self-normalized IS). The following figure shows the weighted samples obtained when using IS to estimate a 2d Gaussian target distribution :math:`p`, sampling from a uniform proposal distribution :math:`q`.

.. image:: _static/SampleMethods_IS_samples.png
   :scale: 40 %
   :alt: IS weighted samples
   :align: center
   
   
.. autoclass:: UQpy.SampleMethods.IS
   :members:
   
.. [1] Gelman et al., "Bayesian data analysis", Chapman and Hall/CRC, 2013
.. [2] R.C. Smith, "Uncertainty Quantification - Theory, Implementation and Applications", CS&E, 2014


.. toctree::
    :maxdepth: 2



	
	