
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>SampleMethods &#8212; UQpy v3.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Transformations" href="transformations.html" />
    <link rel="prev" title="Distributions" href="distributions.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="samplemethods">
<span id="id1"></span><h1>SampleMethods<a class="headerlink" href="#samplemethods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="mcs">
<h2>MCS<a class="headerlink" href="#mcs" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">MCS</span></code> class generates random samples from a specified probability distribution(s).  The <code class="docutils literal notranslate"><span class="pre">MCS</span></code> class utilizes the <code class="docutils literal notranslate"><span class="pre">Distributions</span></code> class to define probability distributions.  The advantage of using the <code class="docutils literal notranslate"><span class="pre">MCS</span></code> class for <code class="docutils literal notranslate"><span class="pre">UQpy</span></code> operations, as opposed to simply generating samples with the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> package, is that it allows building an object containing the samples and their distributions for integration with other <code class="docutils literal notranslate"><span class="pre">UQpy</span></code> modules.</p>
<div class="section" id="class-descriptions">
<h3>Class Descriptions<a class="headerlink" href="#class-descriptions" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="lhs">
<h2>LHS<a class="headerlink" href="#lhs" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">LHS</span></code> class generates random samples from a specified probability distribution(s) using Latin hypercube sampling. LHS has the advantage that the samples generated are uniformly distributed over each marginal distribution. LHS is perfomed by dividing the range of each random variable into N bins with equal probability mass, where N is the required number of samples, generating one sample per bin, and then randomly pairing the samples.</p>
<div class="section" id="adding-new-latin-hypercube-design-criteria">
<h3>Adding New Latin Hypercube Design Criteria<a class="headerlink" href="#adding-new-latin-hypercube-design-criteria" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">LHS</span></code> class offers a variety of methods for pairing the samples in a Latin hypercube design. These are specified by the <cite>criterion</cite> parameter (i.e. ‘random’, ‘centered’, ‘minmax’, ‘correlate’). However, adding a new method is straightforward. This is done by creating a new method that contains the algorithm for pairing the samples. This method takes as input the randomly generated samples in equal probability bins in each dimension and returns a set of samples that is paired according to the user’s desired criterion. The user may also pass criterion-specific parameters into the custom method. These parameters are input to the <code class="docutils literal notranslate"><span class="pre">LHS</span></code> class through the <cite>**kwargs</cite>. The output of this function should be a numpy array of at least two-dimensions with the first dimension being the number of samples and the second dimension being the number of variables . An example user-defined criterion is given below:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">lhs_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">lhs_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">order</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">lhs_samples</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>Class Descriptions<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="sts">
<h2>STS<a class="headerlink" href="#sts" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">STS</span></code> class generates random samples from a specified probability distribution(s) using Stratified sampling. It is a variance reduction sampling technique. It aims to distribute random samples on the complete sample space. The sample space is divided into a set of space-filling and disjoint regions, called strata and samples are generated inside each strata.</p>
<div class="section" id="id3">
<h3>Class Descriptions<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="strata">
<h2>Strata<a class="headerlink" href="#strata" title="Permalink to this headline">¶</a></h2>
<p>The <cite>Strata</cite> class is a supporting class for stratified sampling and its variants. The class defines a rectilinear stratification of the unit hypercube. Strata are defined by specifying a stratum origin as the coordinates of the stratum corner nearest to the global origin and a stratum width for each dimension.</p>
<div class="section" id="id4">
<h3>Class Descriptions<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="rss">
<h2>RSS<a class="headerlink" href="#rss" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class generated samples randomly or uses gradient-based adaptive approach to reduce the variance of output statistical estimates. The method used to generate samples is define by <cite>runmodel_object</cite> parameter. If, it is not defined then RSS class executes Refined Stratified sampling, otherwise Gradient Enhanced Refined Stratified sampling is executed. Refined Stratified sampling randomly selects the stratum to refine from the strata/cells with maximum weight. Whereas, Gradient Enhaced Refined Stratified sampling selects the strata/cells with maximum stratum variance. This class divides the sample domain using either rectangular stratification or voronoi cells, this is define by the <cite>sample_object</cite> parameter. In case of rectangular stratification, selected strata is divided along the maximum width to define new strata. In case of Voronoi cells, the new sample is drawn from a sub-simplex, which is used for refinement.</p>
<div class="section" id="id5">
<h3>Class Descriptions<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="simplex">
<h2>Simplex<a class="headerlink" href="#simplex" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Simplex</span></code> class generates uniformly distributed sample inside a simplex, whose coordinates are expressed by <span class="math notranslate nohighlight">\(\zeta_k\)</span> and <span class="math notranslate nohighlight">\(n_d\)</span> is the dimension. First, this class generates <span class="math notranslate nohighlight">\(n_d\)</span> independent uniform random variables on [0, 1], i.e. <span class="math notranslate nohighlight">\(r_q\)</span>, then compute samples inside simplex using following equation:</p>
<div class="math notranslate nohighlight">
\[\mathbf{M_{n_d}} = \zeta_0 + \sum_{i=1}^{n_d} \Big{[}\prod_{j=1}^{i} r_{n_d-j+1}^{\frac{1}{n_d-j+1}}\Big{]}(\zeta_i - \zeta_{i-1})\]</div>
<p>The <span class="math notranslate nohighlight">\(M_{n_d}\)</span> is <span class="math notranslate nohighlight">\(n_d\)</span> dimensional array defining the coordinates of new sample.</p>
<a class="reference internal image-reference" href="_images/SampleMethods_Simplex.png"><img alt="Randomly generated point inside a 2-D simplex" class="align-center" src="_images/SampleMethods_Simplex.png" style="width: 339.0px; height: 261.0px;" /></a>
<div class="section" id="id6">
<h3>Class Descriptions<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="akmcs">
<h2>AKMCS<a class="headerlink" href="#akmcs" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> class generates random samples from a specified probability distribution(s) using Adaptive Kriging-Monte Carlo Sampling(AKMCS).</p>
<div class="section" id="adding-new-learning-function">
<h3>Adding New Learning Function<a class="headerlink" href="#adding-new-learning-function" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> class offers a variety of learning function to generate samples adaptively. These are specified by the <cite>learning_function</cite> parameter (i.e. ‘U-Function’, ‘Weighted-U Function’, ‘Expected Feasibility Function’, ‘Expected Improvement Function’ and ‘Expected Global Improvement Fit’). However, adding a new learning function is straightforward. This is done by creating a new method that contains the algorithm for selecting a new samples. This method takes as input the surrogate model and randomly generated monte carlo samples, and returns a set of samples that are selected according to the user’s desired learning function. The output of this function should be a numpy array of samples and a boolean indicating the class to continue or stop. The numpy array of samples should be a two-dimensional array with the first dimension being the number of samples and the second dimension being the number of variables . An example user-defined learning function is given below:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">u_function</span><span class="p">(</span><span class="n">surr</span><span class="p">,</span> <span class="n">pop</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">g</span><span class="p">,</span> <span class="n">sig</span> <span class="o">=</span> <span class="n">surr</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">pop</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">sig</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">pop</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">u</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">sig</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">rows</span> <span class="o">=</span> <span class="n">u</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">indicator</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">min</span><span class="p">(</span><span class="n">u</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">indicator</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">pop</span><span class="p">[</span><span class="n">rows</span><span class="p">,</span> <span class="p">:],</span> <span class="n">indicator</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h3>Class Descriptions<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="mcmc">
<h2>MCMC<a class="headerlink" href="#mcmc" title="Permalink to this headline">¶</a></h2>
<p>The goal of Markov Chain Monte Carlo is to draw samples from some probability distribution <span class="math notranslate nohighlight">\(p(x)=\frac{\tilde{p}(x)}{Z}\)</span>, where <span class="math notranslate nohighlight">\(\tilde{p}(x)\)</span> is known but <span class="math notranslate nohighlight">\(Z\)</span> is hard to compute (this will often be the case when using Bayes’ theorem for instance). In order to do this, the theory of a Markov chain, a stochastic model that describes a sequence of states in which the probability of a state depends only on the previous state, is combined with a Monte Carlo simulation method, see e.g. (<a class="footnote-reference brackets" href="#id12" id="id8">1</a>, <a class="footnote-reference brackets" href="#id13" id="id9">2</a>). More specifically, a Markov Chain is built and sampled from whose stationary distribution is the target distribution <span class="math notranslate nohighlight">\(p(x)\)</span>.  For instance, the Metropolis-Hastings (MH) algorithm goes as follows:</p>
<ul class="simple">
<li><p>initialize with a seed sample <span class="math notranslate nohighlight">\(x_{0}\)</span></p></li>
<li><dl class="simple">
<dt>walk the chain: for <span class="math notranslate nohighlight">\(k=0,...\)</span> do:</dt><dd><ul>
<li><p>sample candidate <span class="math notranslate nohighlight">\(x^{\star} \sim Q(\cdot \vert x_{k})\)</span> for a given Markov transition probability <span class="math notranslate nohighlight">\(Q\)</span></p></li>
<li><p>accept candidate (set <span class="math notranslate nohighlight">\(x_{k+1}=x^{\star}\)</span>) with probability <span class="math notranslate nohighlight">\(\alpha(x^{\star} \vert x_{k})\)</span>, otherwise propagate last sample <span class="math notranslate nohighlight">\(x_{k+1}=x_{k}\)</span>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\alpha(x^{\star} \vert x_{k}):= \min \left\{ \frac{\tilde{p}(x^{\star})}{\tilde{p}(x)}\cdot \frac{Q(x \vert x^{\star})}{Q(x^{\star} \vert x)}, 1 \right\}\]</div>
<p>The transition probability <span class="math notranslate nohighlight">\(Q\)</span> is chosen by the user (see input <cite>proposal</cite> of the MH algorithm, and careful attention must be given to that choice as it plays a major role in the accuracy and efficiency of the algorithm. The following figure shows samples accepted (blue) and rejected (red) when trying to sample from a 2d Gaussian distribution using MH, for different scale parameters of the proposal distribution. If the scale is too small, the space is not well explored; if the scale is too large, many candidate samples will be rejected, yielding a very inefficient algorithm. As a rule of thumb, an acceptance rate of 10%-50% could be targeted (see <cite>Diagnostics</cite> in the <cite>Utilities</cite> module).</p>
<a class="reference internal image-reference" href="_images/SampleMethods_MCMC_samples.png"><img alt="IS weighted samples" class="align-center" src="_images/SampleMethods_MCMC_samples.png" style="width: 740.8000000000001px; height: 256.0px;" /></a>
<p>Finally, samples from the target distribution will be generated only when the chain has converged to its stationary distribution, after a so-called burn-in period. Thus the user would often reject the first few samples (see input <cite>nburn</cite>). Also, the chain yields correlated samples; thus to obtain i.i.d. samples from the target distribution, the user should keep only one out of n samples (see input <cite>jump</cite>). This means that the code will perform in total nburn + jump * N evaluations of the target pdf to yield N i.i.d. samples from the target distribution (for the MH algorithm with a single chain).</p>
<p>The parent class for all MCMC algorithms is the <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">class</span></code>, which defines the inputs that are common to all MCMC algorithms, along with the <code class="docutils literal notranslate"><span class="pre">run</span></code> method that is being called to run the chain. Any given MCMC algorithm is a child class of MCMC that overwrites the main <code class="docutils literal notranslate"><span class="pre">run_one_iteration</span></code> method.</p>
<div class="section" id="adding-new-mcmc-algorithms">
<h3>Adding New MCMC Algorithms<a class="headerlink" href="#adding-new-mcmc-algorithms" title="Permalink to this headline">¶</a></h3>
<p>In order to add a new MCMC algorithm, a user must create a child class of <code class="docutils literal notranslate"><span class="pre">MCMC</span></code>, and overwrite the <code class="docutils literal notranslate"><span class="pre">run_one_iteration</span></code> method that propagates all the chains forward one iteration. Such a new class may use any number of additional inputs compared to the <code class="docutils literal notranslate"><span class="pre">MCMC</span></code> base class. The reader is encouraged to have a look at the <code class="docutils literal notranslate"><span class="pre">MH</span></code> class and its code to better understand how a particular algorithm should fit the general framework.</p>
<p>A useful note is that the user has access to a number of useful attributes / utility methods as the algorithm proceeds, such as:</p>
<ul class="simple">
<li><p>the attribute <code class="docutils literal notranslate"><span class="pre">evaluate_log_target</span></code> (and possibly <code class="docutils literal notranslate"><span class="pre">evaluate_log_target_marginals</span></code> if marginals were provided) is created at initialization. It is a callable that simply evaluates the log-pdf of the target distribution at a given point <cite>x</cite>. It can be called within the code of a new sampler as <code class="docutils literal notranslate"><span class="pre">log_pdf_value</span> <span class="pre">=</span> <span class="pre">self.evaluate_log_target(x)</span></code>.</p></li>
<li><p>the <cite>nsamples</cite> and <cite>nsamples_per_chain</cite> attributes indicate the number of samples that have been stored up to the current iteration (i.e., they are updated dynamically as the algorithm proceeds),</p></li>
<li><p>the <cite>samples</cite> attribute contains all previously stored samples. Cautionary note: <cite>self.samples</cite> also contains trailing zeros, for samples yet to be stored, thus to access all previously stored samples at a given iteration the user must call <code class="docutils literal notranslate"><span class="pre">self.samples[:self.nsamples_per_chain]</span></code>, which will return an <cite>ndarray</cite> of size (self.nsamples_per_chain, self.nchains, self.dimension) ,</p></li>
<li><p>the <cite>log_pdf_values</cite> attribute contains all previously stored log target values. Same cautionary note as above,</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">_update_acceptance_rate</span></code> method updates the <cite>acceptance_rate</cite> attribute of the sampler, given a (list of) boolean(s) indicating if the candidate state(s) were accepted at a given iteration,</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">_check_methods_proposal</span></code> method checks whether a given proposal is adequate (i.e., has <code class="docutils literal notranslate"><span class="pre">rvs</span></code> and <code class="docutils literal notranslate"><span class="pre">log_pdf</span></code>/<code class="docutils literal notranslate"><span class="pre">pdf</span></code> methods).</p></li>
</ul>
</div>
<div class="section" id="id10">
<h3>Class Descriptions<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<div class="section" id="mh">
<h4>MH<a class="headerlink" href="#mh" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="mmh">
<h4>MMH<a class="headerlink" href="#mmh" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="stretch">
<h4>Stretch<a class="headerlink" href="#stretch" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="dram">
<h4>DRAM<a class="headerlink" href="#dram" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="dream">
<h4>DREAM<a class="headerlink" href="#dream" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="is">
<h2>IS<a class="headerlink" href="#is" title="Permalink to this headline">¶</a></h2>
<p>Importance sampling (IS) is based on the idea of sampling from an alternate distribution and reweighting the samples to be representative of the target distribution (perhaps concentrating sampling in certain regions of the input space that are of greater importance). This often enables efficient evaluations of expectations <span class="math notranslate nohighlight">\(E_{ \textbf{x} \sim p} [ f(\textbf{x}) ]\)</span> where <span class="math notranslate nohighlight">\(f( \textbf{x})\)</span> is small outside of a small region of the input space. To this end, a sample <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is drawn from a proposal distribution <span class="math notranslate nohighlight">\(q(\textbf{x})\)</span> and re-weighted to correct for the discrepancy between the sampling distribution <span class="math notranslate nohighlight">\(q\)</span> and the true distribution <span class="math notranslate nohighlight">\(p\)</span>. The weight of the sample is computed as</p>
<div class="math notranslate nohighlight">
\[w(\textbf{x}) = \frac{p(\textbf{x})}{q(\textbf{x})}\]</div>
<p>If <span class="math notranslate nohighlight">\(p\)</span> is only known up to a constant, i.e., one can only evaluate <span class="math notranslate nohighlight">\(\tilde{p}(\textbf{x})\)</span>, where <span class="math notranslate nohighlight">\(p(\textbf{x})=\frac{\tilde{p}(\textbf{x})}{Z}\)</span>, IS can be used by further normalizing the weights (self-normalized IS). The following figure shows the weighted samples obtained when using IS to estimate a 2d Gaussian target distribution <span class="math notranslate nohighlight">\(p\)</span>, sampling from a uniform proposal distribution <span class="math notranslate nohighlight">\(q\)</span>.</p>
<a class="reference internal image-reference" href="_images/SampleMethods_IS_samples.png"><img alt="IS weighted samples" class="align-center" src="_images/SampleMethods_IS_samples.png" style="width: 220.8px; height: 226.4px;" /></a>
<div class="section" id="id11">
<h3>Class Descriptions<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<dl class="footnote brackets">
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id8">1</a></span></dt>
<dd><p>Gelman et al., “Bayesian data analysis”, Chapman and Hall/CRC, 2013</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id9">2</a></span></dt>
<dd><p>R.C. Smith, “Uncertainty Quantification - Theory, Implementation and Applications”, CS&amp;E, 2014</p>
</dd>
</dl>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="index.html">
    <img class="logo" src="_static/logo.jpg" alt="Logo"/>
    
  </a>
</p>



<p class="blurb">Uncertainty quantification with Python </p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=SURG&repo=UQpy&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="runmodel.html">RunModel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">Distributions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">SampleMethods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mcs">MCS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lhs">LHS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sts">STS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#strata">Strata</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rss">RSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#simplex">Simplex</a></li>
<li class="toctree-l2"><a class="reference internal" href="#akmcs">AKMCS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mcmc">MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="#is">IS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformations.html">Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_process.html">StochasticProcess</a></li>
<li class="toctree-l1"><a class="reference internal" href="surrogates.html">Surrogates</a></li>
<li class="toctree-l1"><a class="reference internal" href="reliability.html">Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="dimension_reduction.html">DimensionReduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="news.html">News</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="distributions.html" title="previous chapter">Distributions</a></li>
      <li>Next: <a href="transformations.html" title="next chapter">Transformations</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, SURG, JHU.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.0.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/samplemethods.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/SURG/UQpy" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>