
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>SampleMethods &#8212; UQpy v3.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Transformations" href="transformations.html" />
    <link rel="prev" title="Distributions" href="distributions.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="samplemethods">
<span id="id1"></span><h1>SampleMethods<a class="headerlink" href="#samplemethods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="mcs">
<h2>MCS<a class="headerlink" href="#mcs" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">MCS</span></code> class generates random samples from a specified probability distribution(s).  The <code class="docutils literal notranslate"><span class="pre">MCS</span></code> class utilizes the <code class="docutils literal notranslate"><span class="pre">Distributions</span></code> class to define probability distributions.  The advantage of using the <code class="docutils literal notranslate"><span class="pre">MCS</span></code> class for <code class="docutils literal notranslate"><span class="pre">UQpy</span></code> operations, as opposed to simply generating samples with the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> package, is that it allows building an object containing the samples and their distributions for integration with other <code class="docutils literal notranslate"><span class="pre">UQpy</span></code> modules.</p>
<div class="section" id="class-descriptions">
<h3>Class Descriptions<a class="headerlink" href="#class-descriptions" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="lhs">
<h2>LHS<a class="headerlink" href="#lhs" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">LHS</span></code> class generates random samples from a specified probability distribution(s) using Latin hypercube sampling. LHS has the advantage that the samples generated are uniformly distributed over each marginal distribution. LHS is perfomed by dividing the range of each random variable into N bins with equal probability mass, where N is the required number of samples, generating one sample per bin, and then randomly pairing the samples.</p>
<div class="section" id="adding-new-latin-hypercube-design-criteria">
<h3>Adding New Latin Hypercube Design Criteria<a class="headerlink" href="#adding-new-latin-hypercube-design-criteria" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">LHS</span></code> class offers a variety of methods for pairing the samples in a Latin hypercube design. These are specified by the <cite>criterion</cite> parameter (i.e. ‘random’, ‘centered’, ‘minmax’, ‘correlate’). However, adding a new method is straightforward. This is done by creating a new method that contains the algorithm for pairing the samples. This method takes as input the randomly generated samples in equal probability bins in each dimension and returns a set of samples that is paired according to the user’s desired criterion. The user may also pass criterion-specific parameters into the custom method. These parameters are input to the <code class="docutils literal notranslate"><span class="pre">LHS</span></code> class through the <cite>**kwargs</cite>. The output of this function should be a numpy array of at least two-dimensions with the first dimension being the number of samples and the second dimension being the number of variables . An example user-defined criterion is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">lhs_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">lhs_samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">order</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">lhs_samples</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>Class Descriptions<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="stratified-sampling">
<h2>Stratified Sampling<a class="headerlink" href="#stratified-sampling" title="Permalink to this headline">¶</a></h2>
<p>Stratified sampling is a variance reduction technique that divides the parameter space into a set of disjoint and space-filling strata. Samples are then drawn from these strata in order to improve the space-filling properties of the sample design. Stratified sampling allows for unequally weighted samples, such that a Monte Carlo estimator of the quantity <span class="math notranslate nohighlight">\(E[Y]\)</span> takes the following form:</p>
<div class="math notranslate nohighlight">
\[E[Y] \approx \sum_{i=1}^N w_i Y_i\]</div>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> are the sample weights and <span class="math notranslate nohighlight">\(Y_i\)</span> are the model evaluations. The individual sample weights are computed as:</p>
<div class="math notranslate nohighlight">
\[w_i = \dfrac{V_{i}}{N_{i}}\]</div>
<p>where <span class="math notranslate nohighlight">\(V_{i}\le 1\)</span> is the volume of stratum <span class="math notranslate nohighlight">\(i\)</span> in the unit hypercube (i.e. the probability that a random sample will fall in stratum <span class="math notranslate nohighlight">\(i\)</span>) and <span class="math notranslate nohighlight">\(N_{i}\)</span> is the number of samples drawn from stratum <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">UQpy</span></code> supports several stratified sampling variations that vary from conventional stratified sampling designs to advanced gradient informed methods for adaptive stratified sampling. Stratified sampling capabilities are built in <code class="docutils literal notranslate"><span class="pre">UQpy</span></code> from three sets of classes. These class structures facilitate a highly flexible and varied range of stratified sampling designs that can be extended in a straightforward way. Specifically, the existing classes allow stratification of n-dimensional parameter spaces based on three common spatial discretizations: a rectilinear decomposition into hyper-rectangles (orthotopes), a Voronoi decomposition, and a Delaunay decomposition. The three parent classes are:</p>
<ol class="arabic simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class defines the geometric structure of the stratification of the parameter space and it has three existing subclasses - <code class="docutils literal notranslate"><span class="pre">RectangularStrata</span></code>, <code class="docutils literal notranslate"><span class="pre">VoronoiStrata</span></code>, and <code class="docutils literal notranslate"><span class="pre">DelaunayStrata</span></code> that correspond to geometric decompositions of the parameter space based on rectilinear strata of orthotopes, strata composed of Voronoi cells, and strata composed of Delaunay simplexes respectively.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">STS</span></code> class defines a set of subclasses used to draw samples from strata defined by a <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class object.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class defines a set of subclasses for refinement of <code class="docutils literal notranslate"><span class="pre">STS</span></code> stratified sampling designs.</p></li>
</ol>
<div class="section" id="new-stratified-sampling-methods">
<h3>New Stratified Sampling Methods<a class="headerlink" href="#new-stratified-sampling-methods" title="Permalink to this headline">¶</a></h3>
<p>Extension of the stratified sampling capabilities in <code class="docutils literal notranslate"><span class="pre">UQpy</span></code> can be performed through subclassing from the three main classes. First, the user can define a new geometric decomposition of the parameter space by creating a new subclass of the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class. To draw samples from this  new stratification, the user can define a new subclass of the <code class="docutils literal notranslate"><span class="pre">STS</span></code> class. Finally, to enable refinement of the strata based on any user-specified criteria the user can define a new subclass of the <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class.</p>
<p>In summary:</p>
<p>To implement a new stratified sampling method based on a new stratification, the user must write two new classes:</p>
<ol class="arabic simple">
<li><p>A new subclass of the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class defining the new decomposition.</p></li>
<li><p>A new subclass of the <code class="docutils literal notranslate"><span class="pre">STS</span></code> class to perform the sampling from the newly design <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class.</p></li>
</ol>
<p>To implement a new refined stratified sampling method based on a new stratified, the user must write three new classes:</p>
<ol class="arabic simple">
<li><p>A new subclass of the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class defining the new decomposition.</p></li>
<li><p>A new subclass of the <code class="docutils literal notranslate"><span class="pre">STS</span></code> class to perform the sampling from the newly design <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class.</p></li>
<li><p>A new subclass of the <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class to perform the stratum refinement and subsequent sampling.</p></li>
</ol>
<p>The details of these subclasses and their requirements are outlined in the sections below discussing the respective classes.</p>
</div>
<div class="section" id="strata-class">
<h3>Strata Class<a class="headerlink" href="#strata-class" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class is the parent class that defines the geometric decomposition of the parameter space. All geometric decompositions in the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class are performed on the <cite>n</cite>-dimensional unit <span class="math notranslate nohighlight">\([0, 1]^n\)</span> hypercube. Specific stratifications are performed by subclassing the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class. There are currently three stratifications available in the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class, defined through the subclasses <code class="docutils literal notranslate"><span class="pre">RectangularStrata</span></code>, <code class="docutils literal notranslate"><span class="pre">VoronoiStrata</span></code>, and <code class="docutils literal notranslate"><span class="pre">DelaunayStrata</span></code>.</p>
</div>
<div class="section" id="id3">
<h3>Class Descriptions<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="adding-a-new-strata-class">
<h3>Adding a new <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class<a class="headerlink" href="#adding-a-new-strata-class" title="Permalink to this headline">¶</a></h3>
<p>Adding a new type of stratification requires creating a new subclass of the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class that defines the desired geometric decomposition. This subclass must have a <code class="docutils literal notranslate"><span class="pre">stratify</span></code> method that overwrites the corresponding method in the parent class and performs the stratification.</p>
</div>
<div class="section" id="sts-class">
<h3>STS Class<a class="headerlink" href="#sts-class" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">STS</span></code> class is the parent class for stratified sampling. The various <code class="docutils literal notranslate"><span class="pre">STS</span></code> classes generate random samples from a specified probability distribution(s) using stratified sampling with strata specified by an object of one of the <code class="docutils literal notranslate"><span class="pre">Strata</span></code> classes. The <code class="docutils literal notranslate"><span class="pre">STS</span></code> class currently has three child classes - <code class="docutils literal notranslate"><span class="pre">RectangularSTS</span></code>, <code class="docutils literal notranslate"><span class="pre">VoronoiSTS</span></code>, and <code class="docutils literal notranslate"><span class="pre">DelaunaySTS</span></code> - corresponding to stratified sampling methods based rectangular, Voronoi, and Delaunay strata respectively. The following details these classes.</p>
</div>
<div class="section" id="id4">
<h3>Class Descriptions<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="adding-a-new-sts-class">
<h3>Adding a new <code class="docutils literal notranslate"><span class="pre">STS</span></code> class<a class="headerlink" href="#adding-a-new-sts-class" title="Permalink to this headline">¶</a></h3>
<p>Adding a new stratified sampling method first requires that an appropriate <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class exists. If the new method is based on rectangular, Voronoi, or Delaunay stratification one of the existing <code class="docutils literal notranslate"><span class="pre">Strata</span></code> classes can be used. If it relies on a different type of stratification, then a new <code class="docutils literal notranslate"><span class="pre">Strata</span></code> class must be written first. Next, the new stratified sampling method must be written as a new subclass of the <code class="docutils literal notranslate"><span class="pre">STS</span></code> class containing a <code class="docutils literal notranslate"><span class="pre">create_samplesu01</span></code> method that performs the stratified sampling on the unit hypercube. This method must take input that are consistent with the <code class="docutils literal notranslate"><span class="pre">create_samplesu01</span></code> method described in the <code class="docutils literal notranslate"><span class="pre">STS</span></code> class above.</p>
</div>
</div>
<div class="section" id="refined-stratified-sampling">
<h2>Refined Stratified Sampling<a class="headerlink" href="#refined-stratified-sampling" title="Permalink to this headline">¶</a></h2>
<p>Refined Stratified Sampling (RSS) is a sequential sampling procedure that adaptively refines the stratification of the parameter space to add samples. There are four variations of RSS currently available in <code class="docutils literal notranslate"><span class="pre">UQpy</span></code>. First, the procedure works with either rectangular stratification (i.e. using <code class="docutils literal notranslate"><span class="pre">RectangularStrata</span></code>) or Voronoi stratification (i.e. using <code class="docutils literal notranslate"><span class="pre">VoronoiStrata</span></code>). For each of these, two refinement procedures are available. The first is a randomized algorithm where strata are selected at random according to their probability weight. This algorithm is described in <a class="footnote-reference brackets" href="#id35" id="id5">11</a>. The second is a gradient-enhanced version (so-called GE-RSS) that draws samples in stata that possess both large probability weight and have high variance. This algorithm is described in <a class="footnote-reference brackets" href="#id36" id="id6">12</a>.</p>
<div class="section" id="rss-class">
<h3>RSS Class<a class="headerlink" href="#rss-class" title="Permalink to this headline">¶</a></h3>
<p>All variations of Refined Stratifed Sampling are implemented in the <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class. <code class="docutils literal notranslate"><span class="pre">RSS</span></code> is the parent class that includes all Refined Stratified Sampling algorithms, which are implemented as child class, specifically <code class="docutils literal notranslate"><span class="pre">RectangularRSS</span></code> and <code class="docutils literal notranslate"><span class="pre">VoronoiRSS</span></code>. The details of these classes are provided below.</p>
<p>Extension of the RSS class for new algorithms can be accomplished by adding new a new child class with the appropriate algorithm. Depending on the type of stratification, this may require the additional development of new <code class="docutils literal notranslate"><span class="pre">Strata</span></code> and <code class="docutils literal notranslate"><span class="pre">STS</span></code> classes to accommodate the RSS. This is discussed in more details below.</p>
</div>
<div class="section" id="id7">
<h3>Class Descriptions<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="adding-a-new-rss-class">
<h3>Adding a new <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class<a class="headerlink" href="#adding-a-new-rss-class" title="Permalink to this headline">¶</a></h3>
<p>New refined stratified sampling methods can be implemented by subclassing the <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class. The subclass should inherit inputs from the parent class and may also take additional inputs as necessary. Any <code class="docutils literal notranslate"><span class="pre">RSS</span></code> subclass must have a <code class="docutils literal notranslate"><span class="pre">run_rss</span></code> method that is invoked by the <code class="docutils literal notranslate"><span class="pre">RSS.run</span></code> method. The <code class="docutils literal notranslate"><span class="pre">run_rss</span></code> method is an instance method that should not take any additional arguments and executes the refined stratifed sampling algorithm.</p>
<p>It is noted that any new <code class="docutils literal notranslate"><span class="pre">RSS</span></code> class must have a corresponding <code class="docutils literal notranslate"><span class="pre">Strata</span></code> object that defines the type of stratification and may also require a corresponding <code class="docutils literal notranslate"><span class="pre">STS</span></code> class. New <code class="docutils literal notranslate"><span class="pre">RSS</span></code> algorithms that do not utilize the existing <code class="docutils literal notranslate"><span class="pre">Strata</span></code> classes (<code class="docutils literal notranslate"><span class="pre">RectangularStrata</span></code>, <code class="docutils literal notranslate"><span class="pre">VoronoiStrata</span></code>, or <code class="docutils literal notranslate"><span class="pre">DelaunayStrata</span></code>) will require that a new <a href="#id8"><span class="problematic" id="id9">``</span></a>Strata``subclass be written.</p>
</div>
</div>
<div class="section" id="simplex">
<h2>Simplex<a class="headerlink" href="#simplex" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Simplex</span></code> class generates uniformly distributed samples inside a simplex of dimension <span class="math notranslate nohighlight">\(n_d\)</span>, whose coordinates are expressed by <span class="math notranslate nohighlight">\(\zeta_k\)</span>. First, this class generates <span class="math notranslate nohighlight">\(n_d\)</span> independent uniform random variables on [0, 1], denoted <span class="math notranslate nohighlight">\(r_q\)</span>, then maps them to the simplex as follows:</p>
<div class="math notranslate nohighlight">
\[\mathbf{M_{n_d}} = \zeta_0 + \sum_{i=1}^{n_d} \Big{[}\prod_{j=1}^{i} r_{n_d-j+1}^{\frac{1}{n_d-j+1}}\Big{]}(\zeta_i - \zeta_{i-1})\]</div>
<p>where <span class="math notranslate nohighlight">\(M_{n_d}\)</span> is an <span class="math notranslate nohighlight">\(n_d\)</span> dimensional array defining the coordinates of new sample. This mapping is illustrated below for a two-dimensional simplex.</p>
<a class="reference internal image-reference" href="_images/SampleMethods_Simplex.png"><img alt="Randomly generated point inside a 2-D simplex" class="align-center" src="_images/SampleMethods_Simplex.png" style="width: 339.0px; height: 261.0px;" /></a>
<p>Additional details can be found in <a class="footnote-reference brackets" href="#id32" id="id10">8</a>.</p>
<div class="section" id="id11">
<h3>Class Descriptions<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="akmcs">
<h2>AKMCS<a class="headerlink" href="#akmcs" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> class generates samples adaptively using a specified Kriging-based learning function in a general Adaptive Kriging-Monte Carlo Sampling (AKMCS) framework. Based on the specified learning function, different objectives can be achieved. In particular, the <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> class has learning functions for reliabliity analysis (probability of failure estimation), global optimization, best global fit surrogate models, and can also accept user-defined learning functions for these and other objectives.  Note that the term AKMCS is adopted from <a class="footnote-reference brackets" href="#id27" id="id12">3</a> although the procedure is referred to by different names depending on the specific learning function employed. For example, when applied for optimization the algorithm leverages the expected improvement function and is known under the name Efficient Global Optimization (EGO) <a class="footnote-reference brackets" href="#id28" id="id13">4</a>.</p>
<div class="section" id="learning-functions">
<h3>Learning Functions<a class="headerlink" href="#learning-functions" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> provides a number of built-in learning functions as well as allowing the user to proviee a custom learning function. These learning functions are described below.</p>
<div class="section" id="u-function">
<h4>U-Function<a class="headerlink" href="#u-function" title="Permalink to this headline">¶</a></h4>
<p>The U-function is a learning function adopted for Kriging-based reliability analysis adopted from <a class="footnote-reference brackets" href="#id27" id="id14">3</a>. Given a Kriging model <span class="math notranslate nohighlight">\(\hat{y}(\mathbf{x})\)</span>, point estimator of its standard devaition <span class="math notranslate nohighlight">\(\sigma_{\hat{y}}(\mathbf{x})\)</span>, and a set of learning points <span class="math notranslate nohighlight">\(S\)</span>, the U-function seeks out the point <span class="math notranslate nohighlight">\(\mathbf{x}\in S\)</span> that minimizes the function:</p>
<div class="math notranslate nohighlight">
\[U(\mathbf{x}) = \dfrac{|\hat{y}(\mathbf{x})|}{\sigma_{\hat{y}}(\mathbf{x})}\]</div>
<p>This point can be interpreted as the point in <span class="math notranslate nohighlight">\(S\)</span> where the Kriging model has the highest probabability of incorrectly identifying the sign of the performance function (i.e. incorrectly predicting the safe/fail state of the system).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> then adds the corresponding point to the training set, re-fits the Kriging model and repeats the procedure until the following stopping criterion in met:</p>
<div class="math notranslate nohighlight">
\[\min(U(\mathbf{x})) &gt; \epsilon_u\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_u\)</span> is a user-defined error threshold (typically set to 2).</p>
</div>
<div class="section" id="weighted-u-function">
<h4>Weighted U-Function<a class="headerlink" href="#weighted-u-function" title="Permalink to this headline">¶</a></h4>
<p>The probability weighted U-function is a learning function for reliability analysis adapted from the U-function in <a class="footnote-reference brackets" href="#id29" id="id15">5</a>. It modifies the U-function as follows:</p>
<div class="math notranslate nohighlight">
\[W(\mathbf{x}) = \dfrac{\max_x[p(\mathbf{x})] - p(\mathbf{x})}{\max_x[p(\mathbf{x})]} U(\mathbf{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> is the probability density function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. This has the effect of decreasing the learning function for points that have higher probability of occurrence. Thus, given two points with identical values of <span class="math notranslate nohighlight">\(U(x)\)</span>, the weighted learning function will select the point with higher probability of occurrence.</p>
<p>As with the standard U-function, <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> with the weighted U-function iterates until <span class="math notranslate nohighlight">\(\min(U(\mathbf{x})) &gt; \epsilon_u\)</span> (the same stopping criterion as the U-function).</p>
</div>
<div class="section" id="expected-feasibility-function">
<h4>Expected Feasibility Function<a class="headerlink" href="#expected-feasibility-function" title="Permalink to this headline">¶</a></h4>
<p>The Expected Feasibility Function (EFF) is a learning function for reliability analysis introduced as part of the Efficient Global Reliability Analysis (EGRA) method <a class="footnote-reference brackets" href="#id30" id="id16">6</a>. The EFF provides assesses how well the true value of the peformance function, <span class="math notranslate nohighlight">\(y(\mathbf{x})\)</span>, is expected to satisfy the constraint <span class="math notranslate nohighlight">\(y(\mathbf{x}) = a\)</span> over a region <span class="math notranslate nohighlight">\(a-\epsilon \le y(\mathbf{x}) \le a+\epsilon\)</span>. It is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align} EFF(\mathbf{x}) &amp;= (\hat{y}(\mathbf{x})-a)\bigg[2\Phi\bigg(\dfrac{a-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) - \Phi\bigg(\dfrac{(a-\epsilon)-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) - \Phi\bigg(\dfrac{(a+\epsilon)-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) \bigg] \\ &amp;-\sigma_{\hat{y}}(\mathbf{x})\bigg[2\phi\bigg(\dfrac{a-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) - \phi\bigg(\dfrac{(a-\epsilon)-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) - \phi\bigg(\dfrac{(a+\epsilon)-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) \bigg] \\ &amp;+ \bigg[ \Phi\bigg(\dfrac{(a+\epsilon)-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) - \Phi\bigg(\dfrac{(a-\epsilon)-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) \bigg] \end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> are the standard normal cdf and pdf, respectively. For reliabilty, <span class="math notranslate nohighlight">\(a=0\)</span>, and it is suggest to use <span class="math notranslate nohighlight">\(\epsilon=2\sigma_{\hat{y}}^2\)</span>.</p>
<p>At each iteration, the new point that is selected is the point that maximizes the EFF and iterations continue until</p>
<div class="math notranslate nohighlight">
\[\max_x(EFF(\mathbf{x})) &lt; \epsilon_{eff}\]</div>
</div>
<div class="section" id="expected-improvement-function">
<h4>Expected Improvement Function<a class="headerlink" href="#expected-improvement-function" title="Permalink to this headline">¶</a></h4>
<p>The Expected Improvement Function (EIF) is a Kriging-based learning function for global optimization introduced as part of the Efficient Global Optimization (EGO) method in <a class="footnote-reference brackets" href="#id28" id="id17">4</a>. The EIF seeks to find the global minimum of a function. It searches the space by placing samples at locations that maximize the expected improvement, where the improvement is defined as <span class="math notranslate nohighlight">\(I(\mathbf{x})=\max(y_{min}-y(\mathbf{x}), 0)\)</span>, where the model response <span class="math notranslate nohighlight">\(y(\mathbf{x})\)</span> is assumed to be a Gaussian random variable and <span class="math notranslate nohighlight">\(y_{min}\)</span> is the current minimum model response. The EIF is then expressed as:</p>
<div class="math notranslate nohighlight">
\[EIF(\mathbf{x}) = E[I(\mathbf{x})] = (y_{min}-\hat{y}(\mathbf{x})) \Phi \bigg(\dfrac{y_{min}-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg) + \sigma_{\hat{y}}(\mathbf{x})\phi \bigg(\dfrac{y_{min}-\hat{y}(\mathbf{x})}{\sigma_{\hat{y}}(\mathbf{x})} \bigg)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> are the standard normal cdf and pdf, respectively.</p>
<p>At each iteration, the EGO algorithm selects the point in the learning set that maximizes the EIF. The algorithm continues until the maximum number of iterations or until:</p>
<div class="math notranslate nohighlight">
\[\dfrac{EIF(\mathbf{x})}{|y_{min}|} &lt; \epsilon_{eif}.\]</div>
<p>Typically a value of 0.01 is used for <span class="math notranslate nohighlight">\(\epsilon_{eif}\)</span>.</p>
</div>
<div class="section" id="expected-improvement-for-global-fit">
<h4>Expected Improvement for Global Fit<a class="headerlink" href="#expected-improvement-for-global-fit" title="Permalink to this headline">¶</a></h4>
<p>The Expected Improvement for Global Fit (EIGF) learning function aims to build the surrogate model that is the best global representation of model. It was introduced in <a class="footnote-reference brackets" href="#id31" id="id18">7</a>. It aims to balance between even space-filling design and sampling in regions of high variation and is given by:</p>
<div class="math notranslate nohighlight">
\[EIGF(\mathbf{x}) = (\hat{y}(\mathbf{x}) - y(\mathbf{x}_*))^2 + \sigma_{\hat{y}}(\mathbf{x})^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_*\)</span> is the point in the training set closest in distance to the point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(y(\mathbf{x}_*)\)</span> is the model response at that point.</p>
<p>No stopping criterion is suggested by the authors of <a class="footnote-reference brackets" href="#id31" id="id19">7</a>, thus its implementation in <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> uses a fixed number of iterations.</p>
</div>
<div class="section" id="user-defined-learning-functions">
<h4>User-Defined Learning Functions<a class="headerlink" href="#user-defined-learning-functions" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">AKMCS</span></code> class also allows new, user-defined learning functions to be specified in a straightforward way. This is done by creating a new method that contains the algorithm for selecting a new samples. This method takes as input the surrogate model, the randomly generated learning points, the number of points to be added in each iteration, any requisite parameters including a stopping criterion, existing samples, model evaluate at samples and distribution object. It returns a set of samples that are selected according to the user’s desired learning function and the corresponding learning function values. The outputs of this function should be (1) a numpy array of samples to be added; (2) the learning function values at the new sample points, and (3) a boolean stopping criterion indicating whether the iterations should continue (<cite>False</cite>) or stop (<cite>True</cite>). The numpy array of samples should be a two-dimensional array with the first dimension being the number of samples and the second dimension being the number of variables. An example user-defined learning function is given below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">u_function</span><span class="p">(</span><span class="n">surr</span><span class="p">,</span> <span class="n">pop</span><span class="p">,</span> <span class="n">n_add</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">qoi</span><span class="p">,</span> <span class="n">dist_object</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">g</span><span class="p">,</span> <span class="n">sig</span> <span class="o">=</span> <span class="n">surr</span><span class="p">(</span><span class="n">pop</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">pop</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">sig</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">pop</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">u</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">sig</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">rows</span> <span class="o">=</span> <span class="n">u</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="n">n_add</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">new_samples</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">rows</span><span class="p">,</span> <span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">u_lf</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="n">rows</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">indicator</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">min</span><span class="p">(</span><span class="n">u</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;u_stop&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">indicator</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">new_samples</span><span class="p">,</span> <span class="n">u_lf</span><span class="p">,</span> <span class="n">indicator</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id20">
<h3>Class Descriptions<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="mcmc">
<h2>MCMC<a class="headerlink" href="#mcmc" title="Permalink to this headline">¶</a></h2>
<p>The goal of Markov Chain Monte Carlo is to draw samples from some probability distribution <span class="math notranslate nohighlight">\(p(x)=\frac{\tilde{p}(x)}{Z}\)</span>, where <span class="math notranslate nohighlight">\(\tilde{p}(x)\)</span> is known but <span class="math notranslate nohighlight">\(Z\)</span> is hard to compute (this will often be the case when using Bayes’ theorem for instance). In order to do this, the theory of a Markov chain, a stochastic model that describes a sequence of states in which the probability of a state depends only on the previous state, is combined with a Monte Carlo simulation method, see e.g. (<a class="footnote-reference brackets" href="#id25" id="id21">1</a>, <a class="footnote-reference brackets" href="#id26" id="id22">2</a>). More specifically, a Markov Chain is built and sampled from whose stationary distribution is the target distribution <span class="math notranslate nohighlight">\(p(x)\)</span>.  For instance, the Metropolis-Hastings (MH) algorithm goes as follows:</p>
<ul class="simple">
<li><p>initialize with a seed sample <span class="math notranslate nohighlight">\(x_{0}\)</span></p></li>
<li><dl class="simple">
<dt>walk the chain: for <span class="math notranslate nohighlight">\(k=0,...\)</span> do:</dt><dd><ul>
<li><p>sample candidate <span class="math notranslate nohighlight">\(x^{\star} \sim Q(\cdot \vert x_{k})\)</span> for a given Markov transition probability <span class="math notranslate nohighlight">\(Q\)</span></p></li>
<li><p>accept candidate (set <span class="math notranslate nohighlight">\(x_{k+1}=x^{\star}\)</span>) with probability <span class="math notranslate nohighlight">\(\alpha(x^{\star} \vert x_{k})\)</span>, otherwise propagate last sample <span class="math notranslate nohighlight">\(x_{k+1}=x_{k}\)</span>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\alpha(x^{\star} \vert x_{k}):= \min \left\{ \frac{\tilde{p}(x^{\star})}{\tilde{p}(x)}\cdot \frac{Q(x \vert x^{\star})}{Q(x^{\star} \vert x)}, 1 \right\}\]</div>
<p>The transition probability <span class="math notranslate nohighlight">\(Q\)</span> is chosen by the user (see input <cite>proposal</cite> of the MH algorithm, and careful attention must be given to that choice as it plays a major role in the accuracy and efficiency of the algorithm. The following figure shows samples accepted (blue) and rejected (red) when trying to sample from a 2d Gaussian distribution using MH, for different scale parameters of the proposal distribution. If the scale is too small, the space is not well explored; if the scale is too large, many candidate samples will be rejected, yielding a very inefficient algorithm. As a rule of thumb, an acceptance rate of 10%-50% could be targeted (see <cite>Diagnostics</cite> in the <cite>Utilities</cite> module).</p>
<a class="reference internal image-reference" href="_images/SampleMethods_MCMC_samples.png"><img alt="IS weighted samples" class="align-center" src="_images/SampleMethods_MCMC_samples.png" style="width: 740.8000000000001px; height: 256.0px;" /></a>
<p>Finally, samples from the target distribution will be generated only when the chain has converged to its stationary distribution, after a so-called burn-in period. Thus the user would often reject the first few samples (see input <cite>nburn</cite>). Also, the chain yields correlated samples; thus to obtain i.i.d. samples from the target distribution, the user should keep only one out of n samples (see input <cite>jump</cite>). This means that the code will perform in total nburn + jump * N evaluations of the target pdf to yield N i.i.d. samples from the target distribution (for the MH algorithm with a single chain).</p>
<p>The parent class for all MCMC algorithms is the <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">class</span></code>, which defines the inputs that are common to all MCMC algorithms, along with the <code class="docutils literal notranslate"><span class="pre">run</span></code> method that is being called to run the chain. Any given MCMC algorithm is a child class of MCMC that overwrites the main <code class="docutils literal notranslate"><span class="pre">run_one_iteration</span></code> method.</p>
<div class="section" id="adding-new-mcmc-algorithms">
<h3>Adding New MCMC Algorithms<a class="headerlink" href="#adding-new-mcmc-algorithms" title="Permalink to this headline">¶</a></h3>
<p>In order to add a new MCMC algorithm, a user must create a child class of <code class="docutils literal notranslate"><span class="pre">MCMC</span></code>, and overwrite the <code class="docutils literal notranslate"><span class="pre">run_one_iteration</span></code> method that propagates all the chains forward one iteration. Such a new class may use any number of additional inputs compared to the <code class="docutils literal notranslate"><span class="pre">MCMC</span></code> base class. The reader is encouraged to have a look at the <code class="docutils literal notranslate"><span class="pre">MH</span></code> class and its code to better understand how a particular algorithm should fit the general framework.</p>
<p>A useful note is that the user has access to a number of useful attributes / utility methods as the algorithm proceeds, such as:</p>
<ul class="simple">
<li><p>the attribute <code class="docutils literal notranslate"><span class="pre">evaluate_log_target</span></code> (and possibly <code class="docutils literal notranslate"><span class="pre">evaluate_log_target_marginals</span></code> if marginals were provided) is created at initialization. It is a callable that simply evaluates the log-pdf of the target distribution at a given point <cite>x</cite>. It can be called within the code of a new sampler as <code class="docutils literal notranslate"><span class="pre">log_pdf_value</span> <span class="pre">=</span> <span class="pre">self.evaluate_log_target(x)</span></code>.</p></li>
<li><p>the <cite>nsamples</cite> and <cite>nsamples_per_chain</cite> attributes indicate the number of samples that have been stored up to the current iteration (i.e., they are updated dynamically as the algorithm proceeds),</p></li>
<li><p>the <cite>samples</cite> attribute contains all previously stored samples. Cautionary note: <cite>self.samples</cite> also contains trailing zeros, for samples yet to be stored, thus to access all previously stored samples at a given iteration the user must call <code class="docutils literal notranslate"><span class="pre">self.samples[:self.nsamples_per_chain]</span></code>, which will return an <cite>ndarray</cite> of size (self.nsamples_per_chain, self.nchains, self.dimension) ,</p></li>
<li><p>the <cite>log_pdf_values</cite> attribute contains all previously stored log target values. Same cautionary note as above,</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">_update_acceptance_rate</span></code> method updates the <cite>acceptance_rate</cite> attribute of the sampler, given a (list of) boolean(s) indicating if the candidate state(s) were accepted at a given iteration,</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">_check_methods_proposal</span></code> method checks whether a given proposal is adequate (i.e., has <code class="docutils literal notranslate"><span class="pre">rvs</span></code> and <code class="docutils literal notranslate"><span class="pre">log_pdf</span></code>/<code class="docutils literal notranslate"><span class="pre">pdf</span></code> methods).</p></li>
</ul>
</div>
<div class="section" id="id23">
<h3>Class Descriptions<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<div class="section" id="mh">
<h4>MH<a class="headerlink" href="#mh" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="mmh">
<h4>MMH<a class="headerlink" href="#mmh" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="stretch">
<h4>Stretch<a class="headerlink" href="#stretch" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="dram">
<h4>DRAM<a class="headerlink" href="#dram" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="dream">
<h4>DREAM<a class="headerlink" href="#dream" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="is">
<h2>IS<a class="headerlink" href="#is" title="Permalink to this headline">¶</a></h2>
<p>Importance sampling (IS) is based on the idea of sampling from an alternate distribution and reweighting the samples to be representative of the target distribution (perhaps concentrating sampling in certain regions of the input space that are of greater importance). This often enables efficient evaluations of expectations <span class="math notranslate nohighlight">\(E_{ \textbf{x} \sim p} [ f(\textbf{x}) ]\)</span> where <span class="math notranslate nohighlight">\(f( \textbf{x})\)</span> is small outside of a small region of the input space. To this end, a sample <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is drawn from a proposal distribution <span class="math notranslate nohighlight">\(q(\textbf{x})\)</span> and re-weighted to correct for the discrepancy between the sampling distribution <span class="math notranslate nohighlight">\(q\)</span> and the true distribution <span class="math notranslate nohighlight">\(p\)</span>. The weight of the sample is computed as</p>
<div class="math notranslate nohighlight">
\[w(\textbf{x}) = \frac{p(\textbf{x})}{q(\textbf{x})}\]</div>
<p>If <span class="math notranslate nohighlight">\(p\)</span> is only known up to a constant, i.e., one can only evaluate <span class="math notranslate nohighlight">\(\tilde{p}(\textbf{x})\)</span>, where <span class="math notranslate nohighlight">\(p(\textbf{x})=\frac{\tilde{p}(\textbf{x})}{Z}\)</span>, IS can be used by further normalizing the weights (self-normalized IS). The following figure shows the weighted samples obtained when using IS to estimate a 2d Gaussian target distribution <span class="math notranslate nohighlight">\(p\)</span>, sampling from a uniform proposal distribution <span class="math notranslate nohighlight">\(q\)</span>.</p>
<a class="reference internal image-reference" href="_images/SampleMethods_IS_samples.png"><img alt="IS weighted samples" class="align-center" src="_images/SampleMethods_IS_samples.png" style="width: 220.8px; height: 226.4px;" /></a>
<div class="section" id="id24">
<h3>Class Descriptions<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<dl class="footnote brackets">
<dt class="label" id="id25"><span class="brackets"><a class="fn-backref" href="#id21">1</a></span></dt>
<dd><p>Gelman et al., “Bayesian data analysis”, Chapman and Hall/CRC, 2013</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id22">2</a></span></dt>
<dd><p>R.C. Smith, “Uncertainty Quantification - Theory, Implementation and Applications”, CS&amp;E, 2014</p>
</dd>
<dt class="label" id="id27"><span class="brackets">3</span><span class="fn-backref">(<a href="#id12">1</a>,<a href="#id14">2</a>)</span></dt>
<dd><ol class="upperalpha simple" start="2">
<li><p>Echard, N. Gayton and M. Lemaire, “AK-MCS: An active learning reliability method combining Kriging and Monte Carlo Simulation”, Structural Safety, Pages 145-154, 2011.</p></li>
</ol>
</dd>
<dt class="label" id="id28"><span class="brackets">4</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Jones, D. R., Schonlau, M., &amp; Welch, W. J. “Efficient global optimization of expensive black-box functions.” Journal of Global optimization, 13(4), 455-492, 1998.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id15">5</a></span></dt>
<dd><p>V.S. Sundar and Shields, M.D. “Reliablity analysis using adaptive Kriging surrogates and multimodel inference.” ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems. Part A: Civil Engineering. 5(2): 04019004, 2019.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id16">6</a></span></dt>
<dd><p>B.J. Bichon, M.S. Eldred, L.P. Swiler, S. Mahadevan, and J.M. McFarland. “Efficient global reliablity analysis for nonlinear implicit performance functions.” AIAA Journal. 46(10) 2459-2468, (2008).</p>
</dd>
<dt class="label" id="id31"><span class="brackets">7</span><span class="fn-backref">(<a href="#id18">1</a>,<a href="#id19">2</a>)</span></dt>
<dd><p>C.Q. Lam. “Sequential adaptive designs in computer experiments for response surface model fit.” PhD diss., The Ohio State University, 2008.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id10">8</a></span></dt>
<dd><ol class="upperalpha simple" start="23">
<li><ol class="upperalpha simple" start="14">
<li><p>Edeling, R. P. Dwight, P. Cinnella, “Simplex-stochastic collocation method with improved scalability”, Journal of Computational Physics, 310:301–328, 2016.</p></li>
</ol>
</li>
</ol>
</dd>
<dt class="label" id="id33"><span class="brackets">9</span></dt>
<dd><ol class="upperalpha simple" start="11">
<li><p>Tocher. “The art of simulation.” The English Universities Press, London, UK; 1963.</p></li>
</ol>
</dd>
<dt class="label" id="id34"><span class="brackets">10</span></dt>
<dd><p>M.D. Shields, K. Teferra, A. Hapij, and R.P. Daddazio, “Refined Stratified Sampling for efficient Monte Carlo based uncertainty quantification,” Reliability Engineering and System Safety,vol.142, pp.310-325,2015.</p>
</dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id5">11</a></span></dt>
<dd><p>M.D. Shields, K. Teferra, A. Hapij, and R.P. Daddazio, “Refined stratified sampling for efficient Monte Carlo based uncertainty quantification.” Reliability Engineering &amp; System Safety 142 (2015): 310-325.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id6">12</a></span></dt>
<dd><p>M.D. Shields, “Adaptive Monte Carlo analysis for strongly nonlinear stochastic systems.” Reliability Engineering &amp; System Safety 175 (2018): 207-224.</p>
</dd>
</dl>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="index.html">
    <img class="logo" src="_static/logo.jpg" alt="Logo"/>
    
  </a>
</p>



<p class="blurb">Uncertainty quantification with Python </p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=SURG&repo=UQpy&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="runmodel.html">RunModel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">Distributions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">SampleMethods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mcs">MCS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lhs">LHS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#refined-stratified-sampling">Refined Stratified Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#simplex">Simplex</a></li>
<li class="toctree-l2"><a class="reference internal" href="#akmcs">AKMCS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mcmc">MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="#is">IS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="transformations.html">Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="stochastic_process.html">StochasticProcess</a></li>
<li class="toctree-l1"><a class="reference internal" href="surrogates.html">Surrogates</a></li>
<li class="toctree-l1"><a class="reference internal" href="reliability.html">Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="dimension_reduction.html">DimensionReduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="news.html">News</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="distributions.html" title="previous chapter">Distributions</a></li>
      <li>Next: <a href="transformations.html" title="next chapter">Transformations</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Michael D. Shields.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/samplemethods.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/SURG/UQpy" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>