{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Selection\n",
    "\n",
    "Authors: Yuchen Zhou and Audrey Olivier - 12/10/2018 <br>\n",
    "Last modified by Audrey Olivier on 12/13/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review of Bayesian model selection\n",
    "\n",
    "The problem of model selection consists in determining which model(s) best explain the available data $D$, given a set of candidate models $m_{1:M}$. Each model $m_{j}$ is parameterized by a set of parameters $\\theta_{m_{j}} \\in \\Theta_{m_{j}}$, to be estimated based on data. In the Bayesian framework, model selection is perfomed by computing the posterior probability of each model $m_{j}$ using Bayes' theorem:\n",
    "\n",
    "$$P(m_{j} \\vert D) = \\frac{p(D \\vert m_{j})P(m_{j})}{\\sum_{j=1}^{M} P(D \\vert m_{j})P(m_{j})}$$\n",
    "\n",
    "where $P(m_{j})$ is the prior assigned to model $m_{j}$ and $P(D \\vert m_{j})$ is the model evidence, also called marginal likelihood.\n",
    "\n",
    "$$ p(D \\vert m_{j}) = \\int_{\\Theta_{m_{j}}} p(D \\vert m_{j}, \\theta_{m_{j}}) p(\\theta_{m_{j}} \\vert m_{j}) d\\theta_{m_{j}} $$\n",
    "\n",
    "where $p(\\theta_{m_{j}} \\vert m_{j})$ is the prior assigned to the parameter vector of model $m_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical example\n",
    "\n",
    "In the following we present an example for which the posterior pdf of the parameters, evidences and model probabilities can be computed analytically. We drop the $m_{j}$ subscript when referring to model parameters for simplicity. Three models are considered (the domain $x$ is fixed and consists in 50 equally spaced points):\n",
    "\\begin{align*}\n",
    "m_{linear}:& \\quad y = \\theta_{0} x + \\epsilon \\\\\n",
    "m_{quadratic}:& \\quad y = \\theta_{0} x + \\theta_{1} x^2 + \\epsilon \\\\\n",
    "m_{cubic}:& \\quad y = \\theta_{0} x + \\theta_{1} x^2+ \\theta_{2} x^3 + \\epsilon \\\\\n",
    "\\end{align*}\n",
    "\n",
    "All three models can be written in a compact form as $y=X \\theta + \\epsilon$, where $X$ contains the necessary powers of $x$. For all three models, the prior is chosen to be Gaussian, $p(\\theta) = N(\\cdot, \\theta_{prior}, \\Sigma_{prior})  $, and so is the noise $\\epsilon \\sim N(\\cdot; 0, \\sigma_{n}^{2} I)$. Then the posterior of the parameters can be computed analytically as:\n",
    "\n",
    "\\begin{align*}\n",
    "& p(\\theta \\vert D={x,y}) =  N(\\cdot; \\theta_{post}(D), \\Sigma_{post}(D)) \\\\\n",
    "& \\theta_{post}(D) = \\left( \\frac{1}{\\sigma_{n}^{2}}X^{T}X + \\Sigma_{prior}^{-1} \\right)^{-1} \\left(\\frac{1}{\\sigma_{n}^{2}}X^{T}y+\\Sigma^{-1}\\theta_{prior} \\right) \\\\\n",
    "& \\Sigma_{post}(D) = \\left( \\frac{1}{\\sigma_{n}^{2}}X^{T}X + \\Sigma_{prior}^{-1} \\right)^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Then the evidence of each model can be computed as \n",
    "\n",
    "$$ p(D) = \\frac{p(D \\vert \\theta)p(\\theta)}{p(\\theta \\vert D)} $$\n",
    "where $p(D \\vert \\theta) = N(\\cdot; X\\theta, \\sigma_{n}^{2} I)$, $p(\\theta) = N(\\cdot, \\theta_{prior}, \\Sigma_{prior}) $ and $p(\\theta \\vert D) = N(\\cdot, \\theta_{post}(D), \\Sigma_{post}(D))$. This formula can be computed at any point $\\theta$.\n",
    "\n",
    "### Generate data from the quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from UQpy.Inference import *\n",
    "from UQpy.RunModel import RunModel # required to run the quadratic model\n",
    "from sklearn.neighbors import KernelDensity # for the plots\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "[  1.68443956e-01   3.23377297e-01   1.61412723e+00   3.09578314e+00\n",
      "   2.33435386e+00   3.80180868e+00   4.15161476e+00   5.98375606e+00\n",
      "   7.80701456e+00   7.26930083e+00   1.11722929e+01   1.21445941e+01\n",
      "   1.43736715e+01   1.61677918e+01   2.01592462e+01   2.15401036e+01\n",
      "   2.46469275e+01   2.76768394e+01   3.17499863e+01   3.56083132e+01\n",
      "   3.69968372e+01   4.21928700e+01   4.38248416e+01   4.88103971e+01\n",
      "   5.12975930e+01   5.64823141e+01   6.08208445e+01   6.61695470e+01\n",
      "   6.97199585e+01   7.68601003e+01   8.20631947e+01   8.43504068e+01\n",
      "   9.06800293e+01   9.69681202e+01   1.04522875e+02   1.07612268e+02\n",
      "   1.14434110e+02   1.21318370e+02   1.27484935e+02   1.35638621e+02\n",
      "   1.41720183e+02   1.47721792e+02   1.54746998e+02   1.63080678e+02\n",
      "   1.69199636e+02   1.76060250e+02   1.86773302e+02   1.92493799e+02\n",
      "   2.02235856e+02   2.10199971e+02]\n"
     ]
    }
   ],
   "source": [
    "# Generate data from a quadratic function\n",
    "import random\n",
    "random.seed(8) #set seed for reproducibility\n",
    "from scipy.stats import multivariate_normal\n",
    "param_true = np.array([1.0, 2.0]).reshape(1, -1)\n",
    "var_n = 1\n",
    "error_covariance = var_n*np.eye(50)\n",
    "print(param_true.shape)\n",
    "\n",
    "z = RunModel(samples=param_true, model_script='pfn_models.py', model_object_name = 'model_quadratic', \n",
    "             var_names = ['theta_1', 'theta_2'])\n",
    "data = z.qoi_list[0].reshape((-1,))+multivariate_normal.rvs(mean=None, cov=error_covariance, size=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models, compute the true values of the evidence.\n",
    "\n",
    "For all three models, a Gaussian prior is chosen for the parameters, with mean and covariance matrix of the appropriate dimensions. Each model is given prior probability $P(m_{j}) = 1/3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model_names = ['model_linear', 'model_quadratic', 'model_cubic']\n",
    "model_n_params = [1, 2, 3]\n",
    "model_prior_means = [[0], [0,0], [0,0,0]]\n",
    "model_prior_stds = [[10], [1,1], [1,2,0.25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior mean and covariance for model_linear\n",
      "[ 16.11858731] [[ 0.00059394]]\n",
      "evidence for model_linear= 0.0\n",
      "\n",
      "posterior mean and covariance for model_quadratic\n",
      "[ 0.99382423  1.99640345] [[ 0.00941698 -0.00116538]\n",
      " [-0.00116538  0.00015392]]\n",
      "evidence for model_quadratic= 3.9052574240850886e-34\n",
      "\n",
      "posterior mean and covariance for model_cubic\n",
      "[ 1.29285743  1.89676101  0.0074434 ] [[  5.60383740e-02  -1.66457308e-02   1.15371992e-03]\n",
      " [ -1.66457308e-02   5.29411877e-03  -3.83090725e-04]\n",
      " [  1.15371992e-03  -3.83090725e-04   2.85512405e-05]]\n",
      "evidence for model_cubic= 4.909879048884739e-35\n",
      "\n",
      "posterior probabilities of all three models\n",
      "[0.0, 0.88831653646519537, 0.11168346353480459]\n"
     ]
    }
   ],
   "source": [
    "evidences = []\n",
    "model_posterior_means = []\n",
    "model_posterior_stds = []\n",
    "for n, model in enumerate(model_names):\n",
    "    # compute matrix X\n",
    "    X = np.linspace(0, 10, 50).reshape((-1,1))\n",
    "    if n == 1: # quadratic model\n",
    "        X = np.concatenate([X, X**2], axis=1)\n",
    "    if n == 2: # cubic model\n",
    "        X = np.concatenate([X, X**2, X**3], axis=1)\n",
    "\n",
    "    # compute posterior pdf\n",
    "    m_prior = np.array(model_prior_means[n]).reshape((-1,1))\n",
    "    S_prior = np.diag(np.array(model_prior_stds[n])**2)\n",
    "    S_posterior = np.linalg.inv(1/var_n*np.matmul(X.T,X)+np.linalg.inv(S_prior))\n",
    "    m_posterior = np.matmul(S_posterior, \n",
    "                            1/var_n*np.matmul(X.T, data.reshape((-1,1)))+np.matmul(np.linalg.inv(S_prior),m_prior))\n",
    "    m_prior = m_prior.reshape((-1,))\n",
    "    m_posterior = m_posterior.reshape((-1,))\n",
    "    model_posterior_means.append(list(m_posterior))\n",
    "    model_posterior_stds.append(list(np.sqrt(np.diag(S_posterior))))\n",
    "    print('posterior mean and covariance for '+model)\n",
    "    print(m_posterior, S_posterior)\n",
    "    \n",
    "    # compute evidence, evaluate the formula at the posterior mean\n",
    "    like_theta = multivariate_normal.pdf(data, mean=np.matmul(X,m_posterior).reshape((-1,)), cov=error_covariance)\n",
    "    prior_theta = multivariate_normal.pdf(m_posterior, mean=m_prior, cov=S_prior)\n",
    "    posterior_theta = multivariate_normal.pdf(m_posterior, mean=m_posterior, cov=S_posterior)\n",
    "    evidence = like_theta*prior_theta/posterior_theta\n",
    "    evidences.append(evidence)\n",
    "    print('evidence for '+model+'= {}\\n'.format(evidence))\n",
    "    \n",
    "# compute the posterior probability of each model\n",
    "tmp = [1/3*evidence for evidence in evidences]\n",
    "model_posterior_probas = [p/sum(tmp) for p in tmp]\n",
    "\n",
    "print('posterior probabilities of all three models')\n",
    "print(model_posterior_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models for use in UQpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "candidate_models = []\n",
    "for n, model_name in enumerate(model_names):\n",
    "    model = Model(n_params=model_n_params[n], model_type='python', \n",
    "                  model_script='pfn_models.py', model_object_name = model_name,\n",
    "                  prior_name = ['normal']*model_n_params[n], \n",
    "                  prior_params = [[m, std] for (m,std) in zip(model_prior_means[n], model_prior_stds[n])],\n",
    "                  error_covariance=error_covariance,\n",
    "                  model_name=model_name)\n",
    "    candidate_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC for all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a88f1b94ac30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      \u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjump\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnburn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_proposal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Normal'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                      \u001b[0mpdf_proposal_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                      \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_prior_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                                      )\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# plot prior, true posterior and estimated posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dimitrisgiovanis/PycharmProjects/UQpy/src/UQpy/Inference.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, sampling_method, pdf_proposal_type, pdf_proposal_scale, pdf_proposal, pdf_proposal_params, algorithm, jump, nsamples, nburn, seed, verbose)\u001b[0m\n\u001b[1;32m    486\u001b[0m                      \u001b[0mpdf_proposal_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf_proposal_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                      \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjump\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnburn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnburn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                      nsamples=self.nsamples, log_pdf_target=self.log_posterior)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dimitrisgiovanis/PycharmProjects/UQpy/src/UQpy/SampleMethods.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dimension, pdf_proposal_type, pdf_proposal_scale, pdf_target, log_pdf_target, pdf_target_params, pdf_target_copula, pdf_target_copula_params, pdf_target_type, algorithm, jump, nsamples, seed, nburn, verbose)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnburn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnburn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf_target_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_mcmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m'Stretch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dimitrisgiovanis/PycharmProjects/UQpy/src/UQpy/SampleMethods.py\u001b[0m in \u001b[0;36minit_mcmc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exit code: Incompatible dimensions in 'seed'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "from scipy.stats import norm\n",
    "\n",
    "bayesMCMC = BayesParameterEstimation(data=data, model=candidate_models[0], sampling_method = 'MCMC', nsamples=1200,\n",
    "                                     algorithm = 'MH', jump=10, nburn=100, pdf_proposal = 'Normal',\n",
    "                                     pdf_proposal_scale = [0.1], \n",
    "                                     seed = model_prior_means[0]\n",
    "                                     )\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "domain_plot = np.linspace(0,20,200)\n",
    "ax.plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[0], scale=model_prior_stds[0]),\n",
    "            label = 'prior', color='green', linestyle='--')\n",
    "ax.plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[0], \n",
    "                                  scale=model_posterior_stds[0]),\n",
    "            label = 'true posterior', color='red', linestyle='-')\n",
    "ax.hist(bayesMCMC.samples[:,0], normed=True, bins=100, label='estimated posterior MCMC')\n",
    "plt.legend()\n",
    "plt.title('MCMC for linear model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quadratic system\n",
    "bayesMCMC = BayesParameterEstimation(data=data, model=candidate_models[1], sampling_method = 'MCMC', nsamples=3500,\n",
    "                                     algorithm = 'MH', jump=10, nburn=100, pdf_proposal = 'Normal',\n",
    "                                     pdf_proposal_scale = [0.1, 0.1], \n",
    "                                     seed = model_prior_means[1]\n",
    "                                     )\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,5))\n",
    "for n_p in range(2):\n",
    "    domain_plot = np.linspace(-0.5,3,200)\n",
    "    ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[1][n_p], scale=model_prior_stds[1][n_p]),\n",
    "                label = 'prior', color='green', linestyle='--')\n",
    "    ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[1][n_p], \n",
    "                                      scale=model_posterior_stds[1][n_p]),\n",
    "                label = 'true posterior', color='red', linestyle='-')\n",
    "    ax[n_p].hist(bayesMCMC.samples[:,n_p], density=True, bins=30, label='estimated posterior MCMC')\n",
    "    ax[n_p].legend()\n",
    "    ax[n_p].set_title('MCMC for quadratic model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cubic system\n",
    "domain_plot = [np.linspace(-0.5,3,200), np.linspace(0,3,200), np.linspace(-0.5,0.5,200)]\n",
    "\n",
    "bayesMCMC = BayesParameterEstimation(data=data, model=candidate_models[2], sampling_method = 'MCMC', nsamples=12000,\n",
    "                                     algorithm = 'MH', jump=30, nburn=500, pdf_proposal = 'Normal',\n",
    "                                     pdf_proposal_scale = [0.15, 0.1, 0.05], \n",
    "                                     seed = model_prior_means[2]\n",
    "                                     )\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,5))\n",
    "for n_p in range(3):\n",
    "    ax[n_p].plot(domain_plot[n_p], norm.pdf(domain_plot[n_p], loc=model_prior_means[2][n_p], \n",
    "                                            scale=model_prior_stds[2][n_p]),\n",
    "                label = 'prior', color='green', linestyle='--')\n",
    "    ax[n_p].plot(domain_plot[n_p], norm.pdf(domain_plot[n_p], loc=model_posterior_means[2][n_p], \n",
    "                                      scale=model_posterior_stds[2][n_p]),\n",
    "                label = 'true posterior', color='red', linestyle='-')\n",
    "    ax[n_p].hist(bayesMCMC.samples[:,n_p], density=True, bins=30, label='estimated posterior MCMC')\n",
    "    ax[n_p].legend()\n",
    "    ax[n_p].set_title('MCMC for cubic model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Model Selection for all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines constants for the MCMC learning part, same as above\n",
    "algos = ['MH']*3\n",
    "proposal_types=['Normal']*3\n",
    "scales = [[0.1],[0.1, 0.1],[0.15, 0.1, 0.05]]\n",
    "nsamples = [1200,3500,12000]\n",
    "nburn = [100,100,500]\n",
    "jump = [10,10,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = BayesModelSelection(data=data, candidate_models=candidate_models, pdf_proposal_type=proposal_types, \n",
    "                                pdf_proposal_scale=scales, algorithm=algos, jump=jump, nsamples=nsamples, nburn=nburn,  \n",
    "                                prior_probabilities=[1./3., 1./3., 1./3.],\n",
    "                                seed=model_prior_means, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Sorted models:')\n",
    "print(selection.sorted_model_names)\n",
    "print('Evidence of sorted models:')\n",
    "print(selection.sorted_evidences)\n",
    "print('Posterior probabilities of sorted models:')\n",
    "print(selection.sorted_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 2, the implementation of BayesModelSelection in UQpy uses the method of the harmonic mean to compute the models' evidence. This method is known to behave quite poorly, in particular it yeidls estimates with large variance. In the problem above, this implementation consistently detects that the quadratic model has the highest model probability, followed by the cubic model. However, the values of the evidence, and thus the model probabilities, are quite off and should not be trusted. Future versions of UQpy will integrate more advanced methods for the estimation of the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
