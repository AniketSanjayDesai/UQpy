{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Selection\n",
    "\n",
    "Authors: Yuchen Zhou and Audrey Olivier - 12/10/2018 <br>\n",
    "Last modified by Audrey Olivier on 12/13/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick review of Bayesian model selection\n",
    "\n",
    "The problem of model selection consists in determining which model(s) best explain the available data $D$, given a set of candidate models $m_{1:M}$. Each model $m_{j}$ is parameterized by a set of parameters $\\theta_{m_{j}} \\in \\Theta_{m_{j}}$, to be estimated based on data. In the Bayesian framework, model selection is perfomed by computing the posterior probability of each model $m_{j}$ using Bayes' theorem:\n",
    "\n",
    "$$P(m_{j} \\vert D) = \\frac{p(D \\vert m_{j})P(m_{j})}{\\sum_{j=1}^{M} P(D \\vert m_{j})P(m_{j})}$$\n",
    "\n",
    "where $P(m_{j})$ is the prior assigned to model $m_{j}$ and $P(D \\vert m_{j})$ is the model evidence, also called marginal likelihood.\n",
    "\n",
    "$$ p(D \\vert m_{j}) = \\int_{\\Theta_{m_{j}}} p(D \\vert m_{j}, \\theta_{m_{j}}) p(\\theta_{m_{j}} \\vert m_{j}) d\\theta_{m_{j}} $$\n",
    "\n",
    "where $p(\\theta_{m_{j}} \\vert m_{j})$ is the prior assigned to the parameter vector of model $m_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical example\n",
    "\n",
    "In the following we present an example for which the posterior pdf of the parameters, evidences and model probabilities can be computed analytically. We drop the $m_{j}$ subscript when referring to model parameters for simplicity. Three models are considered (the domain $x$ is fixed and consists in 50 equally spaced points):\n",
    "\\begin{align*}\n",
    "m_{linear}:& \\quad y = \\theta_{0} x + \\epsilon \\\\\n",
    "m_{quadratic}:& \\quad y = \\theta_{0} x + \\theta_{1} x^2 + \\epsilon \\\\\n",
    "m_{cubic}:& \\quad y = \\theta_{0} x + \\theta_{1} x^2+ \\theta_{2} x^3 + \\epsilon \\\\\n",
    "\\end{align*}\n",
    "\n",
    "All three models can be written in a compact form as $y=X \\theta + \\epsilon$, where $X$ contains the necessary powers of $x$. For all three models, the prior is chosen to be Gaussian, $p(\\theta) = N(\\cdot, \\theta_{prior}, \\Sigma_{prior})  $, and so is the noise $\\epsilon \\sim N(\\cdot; 0, \\sigma_{n}^{2} I)$. Then the posterior of the parameters can be computed analytically as:\n",
    "\n",
    "\\begin{align*}\n",
    "& p(\\theta \\vert D={x,y}) =  N(\\cdot; \\theta_{post}(D), \\Sigma_{post}(D)) \\\\\n",
    "& \\theta_{post}(D) = \\left( \\frac{1}{\\sigma_{n}^{2}}X^{T}X + \\Sigma_{prior}^{-1} \\right)^{-1} \\left(\\frac{1}{\\sigma_{n}^{2}}X^{T}y+\\Sigma^{-1}\\theta_{prior} \\right) \\\\\n",
    "& \\Sigma_{post}(D) = \\left( \\frac{1}{\\sigma_{n}^{2}}X^{T}X + \\Sigma_{prior}^{-1} \\right)^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Then the evidence of each model can be computed as \n",
    "\n",
    "$$ p(D) = \\frac{p(D \\vert \\theta)p(\\theta)}{p(\\theta \\vert D)} $$\n",
    "where $p(D \\vert \\theta) = N(\\cdot; X\\theta, \\sigma_{n}^{2} I)$, $p(\\theta) = N(\\cdot, \\theta_{prior}, \\Sigma_{prior}) $ and $p(\\theta \\vert D) = N(\\cdot, \\theta_{post}(D), \\Sigma_{post}(D))$. This formula can be computed at any point $\\theta$.\n",
    "\n",
    "### Generate data from the quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from UQpy.Inference import *\n",
    "from UQpy.RunModel import RunModel # required to run the quadratic model\n",
    "from sklearn.neighbors import KernelDensity # for the plots\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "[   2.50090337   -0.70597804    1.40981907    0.91038173    1.26014919\n",
      "    3.01571637    4.42462127    5.2333367     9.97571796    9.25970114\n",
      "    9.37983263   13.35599323   13.02698754   17.90308429   19.85877816\n",
      "   21.56427303   24.49611695   27.0333398    29.92287314   32.7076207\n",
      "   38.06634112   42.57237601   43.46637566   49.35469896   53.04517457\n",
      "   56.64429845   63.31427249   66.56729976   69.54430452   77.06033051\n",
      "   80.95889887   84.47871901   90.7257764    97.22851662  102.73583726\n",
      "  109.87675125  115.47298239  122.92623947  128.56891647  134.1507192\n",
      "  140.86226191  148.0957657   155.01321612  161.93286329  169.65827434\n",
      "  177.54051364  185.55340608  193.1836388   201.15973662  211.22125075]\n"
     ]
    }
   ],
   "source": [
    "# Generate data from a quadratic function\n",
    "import random\n",
    "random.seed(8) #set seed for reproducibility\n",
    "from scipy.stats import multivariate_normal\n",
    "param_true = np.array([1.0, 2.0]).reshape(1, -1)\n",
    "var_n = 1\n",
    "error_covariance = var_n*np.eye(50)\n",
    "print(param_true.shape)\n",
    "\n",
    "z = RunModel(samples=param_true, model_script='pfn_models.py', model_object_name = 'model_quadratic', \n",
    "             var_names = ['theta_1', 'theta_2'])\n",
    "data = z.qoi_list[0].reshape((-1,))+multivariate_normal.rvs(mean=None, cov=error_covariance, size=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models, compute the true values of the evidence.\n",
    "\n",
    "For all three models, a Gaussian prior is chosen for the parameters, with mean and covariance matrix of the appropriate dimensions. Each model is given prior probability $P(m_{j}) = 1/3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model_names = ['model_linear', 'model_quadratic', 'model_cubic']\n",
    "model_n_params = [1, 2, 3]\n",
    "model_prior_means = [[0], [0,0], [0,0,0]]\n",
    "model_prior_stds = [[10], [1,1], [1,2,0.25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior mean and covariance for model_linear\n",
      "[ 16.13704247] [[ 0.00059394]]\n",
      "evidence for model_linear= 0.0\n",
      "\n",
      "posterior mean and covariance for model_quadratic\n",
      "[ 1.01415802  1.99615389] [[ 0.00941698 -0.00116538]\n",
      " [-0.00116538  0.00015392]]\n",
      "evidence for model_quadratic= 4.60682785692609e-36\n",
      "\n",
      "posterior mean and covariance for model_cubic\n",
      "[ 1.13354078  1.95616412  0.00299756] [[  5.60383740e-02  -1.66457308e-02   1.15371992e-03]\n",
      " [ -1.66457308e-02   5.29411877e-03  -3.83090725e-04]\n",
      " [  1.15371992e-03  -3.83090725e-04   2.85512405e-05]]\n",
      "evidence for model_cubic= 2.5681568975802695e-37\n",
      "\n",
      "posterior probabilities of all three models\n",
      "[0.0, 0.94719685205279569, 0.052803147947204324]\n"
     ]
    }
   ],
   "source": [
    "evidences = []\n",
    "model_posterior_means = []\n",
    "model_posterior_stds = []\n",
    "for n, model in enumerate(model_names):\n",
    "    # compute matrix X\n",
    "    X = np.linspace(0, 10, 50).reshape((-1,1))\n",
    "    if n == 1: # quadratic model\n",
    "        X = np.concatenate([X, X**2], axis=1)\n",
    "    if n == 2: # cubic model\n",
    "        X = np.concatenate([X, X**2, X**3], axis=1)\n",
    "\n",
    "    # compute posterior pdf\n",
    "    m_prior = np.array(model_prior_means[n]).reshape((-1,1))\n",
    "    S_prior = np.diag(np.array(model_prior_stds[n])**2)\n",
    "    S_posterior = np.linalg.inv(1/var_n*np.matmul(X.T,X)+np.linalg.inv(S_prior))\n",
    "    m_posterior = np.matmul(S_posterior, \n",
    "                            1/var_n*np.matmul(X.T, data.reshape((-1,1)))+np.matmul(np.linalg.inv(S_prior),m_prior))\n",
    "    m_prior = m_prior.reshape((-1,))\n",
    "    m_posterior = m_posterior.reshape((-1,))\n",
    "    model_posterior_means.append(list(m_posterior))\n",
    "    model_posterior_stds.append(list(np.sqrt(np.diag(S_posterior))))\n",
    "    print('posterior mean and covariance for '+model)\n",
    "    print(m_posterior, S_posterior)\n",
    "    \n",
    "    # compute evidence, evaluate the formula at the posterior mean\n",
    "    like_theta = multivariate_normal.pdf(data, mean=np.matmul(X,m_posterior).reshape((-1,)), cov=error_covariance)\n",
    "    prior_theta = multivariate_normal.pdf(m_posterior, mean=m_prior, cov=S_prior)\n",
    "    posterior_theta = multivariate_normal.pdf(m_posterior, mean=m_posterior, cov=S_posterior)\n",
    "    evidence = like_theta*prior_theta/posterior_theta\n",
    "    evidences.append(evidence)\n",
    "    print('evidence for '+model+'= {}\\n'.format(evidence))\n",
    "    \n",
    "# compute the posterior probability of each model\n",
    "tmp = [1/3*evidence for evidence in evidences]\n",
    "model_posterior_probas = [p/sum(tmp) for p in tmp]\n",
    "\n",
    "print('posterior probabilities of all three models')\n",
    "print(model_posterior_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the models for use in UQpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "candidate_models = []\n",
    "for n, model_name in enumerate(model_names):\n",
    "    model = Model(n_params=model_n_params[n], model_type='python', \n",
    "                  model_script='pfn_models.py', model_object_name = model_name,\n",
    "                  prior_name = ['normal']*model_n_params[n], \n",
    "                  prior_params = [[m, std] for (m,std) in zip(model_prior_means[n], model_prior_stds[n])],\n",
    "                  error_covariance=error_covariance,\n",
    "                  model_name=model_name)\n",
    "    candidate_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC for all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unknown property density",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-81fab45279b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   scale=model_posterior_stds[0]),\n\u001b[1;32m     16\u001b[0m             label = 'true posterior', color='red', linestyle='-')\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbayesMCMC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'estimated posterior MCMC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MCMC for linear model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   6387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6388\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6389\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6390\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6391\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             ret = [_update_property(self, k, v)\n\u001b[0;32m--> 885\u001b[0;31m                    for k, v in props.items()]\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meventson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             ret = [_update_property(self, k, v)\n\u001b[0;32m--> 885\u001b[0;31m                    for k, v in props.items()]\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meventson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_property\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m    876\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown property %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Unknown property density"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "from scipy.stats import norm\n",
    "\n",
    "bayesMCMC = BayesParameterEstimation(data=data, model=candidate_models[0], sampling_method = 'MCMC', nsamples=1200,\n",
    "                                     algorithm = 'MH', jump=10, nburn=100, pdf_proposal = 'Normal',\n",
    "                                     pdf_proposal_scale = [0.1], \n",
    "                                     seed = model_prior_means[0]\n",
    "                                     )\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "domain_plot = np.linspace(0,20,200)\n",
    "ax.plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[0], scale=model_prior_stds[0]),\n",
    "            label = 'prior', color='green', linestyle='--')\n",
    "ax.plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[0], \n",
    "                                  scale=model_posterior_stds[0]),\n",
    "            label = 'true posterior', color='red', linestyle='-')\n",
    "ax.hist(bayesMCMC.samples[:,0], density=True, bins=100, label='estimated posterior MCMC')\n",
    "plt.legend()\n",
    "plt.title('MCMC for linear model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic system\n",
    "bayesMCMC = BayesParameterEstimation(data=data, model=candidate_models[1], sampling_method = 'MCMC', nsamples=3500,\n",
    "                                     algorithm = 'MH', jump=10, nburn=100, pdf_proposal = 'Normal',\n",
    "                                     pdf_proposal_scale = [0.1, 0.1], \n",
    "                                     seed = model_prior_means[1]\n",
    "                                     )\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,5))\n",
    "for n_p in range(2):\n",
    "    domain_plot = np.linspace(-0.5,3,200)\n",
    "    ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_prior_means[1][n_p], scale=model_prior_stds[1][n_p]),\n",
    "                label = 'prior', color='green', linestyle='--')\n",
    "    ax[n_p].plot(domain_plot, norm.pdf(domain_plot, loc=model_posterior_means[1][n_p], \n",
    "                                      scale=model_posterior_stds[1][n_p]),\n",
    "                label = 'true posterior', color='red', linestyle='-')\n",
    "    ax[n_p].hist(bayesMCMC.samples[:,n_p], density=True, bins=30, label='estimated posterior MCMC')\n",
    "    ax[n_p].legend()\n",
    "    ax[n_p].set_title('MCMC for quadratic model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cubic system\n",
    "domain_plot = [np.linspace(-0.5,3,200), np.linspace(0,3,200), np.linspace(-0.5,0.5,200)]\n",
    "\n",
    "bayesMCMC = BayesParameterEstimation(data=data, model=candidate_models[2], sampling_method = 'MCMC', nsamples=12000,\n",
    "                                     algorithm = 'MH', jump=30, nburn=500, pdf_proposal = 'Normal',\n",
    "                                     pdf_proposal_scale = [0.15, 0.1, 0.05], \n",
    "                                     seed = model_prior_means[2]\n",
    "                                     )\n",
    "# plot prior, true posterior and estimated posterior\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,5))\n",
    "for n_p in range(3):\n",
    "    ax[n_p].plot(domain_plot[n_p], norm.pdf(domain_plot[n_p], loc=model_prior_means[2][n_p], \n",
    "                                            scale=model_prior_stds[2][n_p]),\n",
    "                label = 'prior', color='green', linestyle='--')\n",
    "    ax[n_p].plot(domain_plot[n_p], norm.pdf(domain_plot[n_p], loc=model_posterior_means[2][n_p], \n",
    "                                      scale=model_posterior_stds[2][n_p]),\n",
    "                label = 'true posterior', color='red', linestyle='-')\n",
    "    ax[n_p].hist(bayesMCMC.samples[:,n_p], density=True, bins=30, label='estimated posterior MCMC')\n",
    "    ax[n_p].legend()\n",
    "    ax[n_p].set_title('MCMC for cubic model')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Model Selection for all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines constants for the MCMC learning part, same as above\n",
    "algos = ['MH']*3\n",
    "proposal_types=['Normal']*3\n",
    "scales = [[0.1],[0.1, 0.1],[0.15, 0.1, 0.05]]\n",
    "nsamples = [1200,3500,12000]\n",
    "nburn = [100,100,500]\n",
    "jump = [10,10,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UQpy: Running Bayesian MS...\n",
      "UQpy: Running MCMC for model model_linear\n",
      "UQpy: Running parameter estimation for candidate model  model_linear\n",
      "UQpy: Parameter estimation analysis completed!\n",
      "UQpy: Running MCMC for model model_quadratic\n",
      "UQpy: Running parameter estimation for candidate model  model_quadratic\n",
      "UQpy: Parameter estimation analysis completed!\n",
      "UQpy: Running MCMC for model model_cubic\n",
      "UQpy: Running parameter estimation for candidate model  model_cubic\n",
      "UQpy: Parameter estimation analysis completed!\n",
      "UQpy: Bayesian MS analysis completed!\n"
     ]
    }
   ],
   "source": [
    "selection = BayesModelSelection(data=data, candidate_models=candidate_models, pdf_proposal_type=proposal_types, \n",
    "                                pdf_proposal_scale=scales, algorithm=algos, jump=jump, nsamples=nsamples, nburn=nburn,  \n",
    "                                prior_probabilities=[1./3., 1./3., 1./3.],\n",
    "                                seed=model_prior_means, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted models:\n",
      "['model_quadratic', 'model_cubic', 'model_linear']\n",
      "Evidence of sorted models:\n",
      "[7.063115637235827e-31, 3.5140937892690253e-31, 0.0]\n",
      "Posterior probabilities of sorted models:\n",
      "[0.6677673999284491, 0.3322326000715509, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print('Sorted models:')\n",
    "print(selection.sorted_model_names)\n",
    "print('Evidence of sorted models:')\n",
    "print(selection.sorted_evidences)\n",
    "print('Posterior probabilities of sorted models:')\n",
    "print(selection.sorted_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 2, the implementation of BayesModelSelection in UQpy uses the method of the harmonic mean to compute the models' evidence. This method is known to behave quite poorly, in particular it yeidls estimates with large variance. In the problem above, this implementation consistently detects that the quadratic model has the highest model probability, followed by the cubic model. However, the values of the evidence, and thus the model probabilities, are quite off and should not be trusted. Future versions of UQpy will integrate more advanced methods for the estimation of the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
